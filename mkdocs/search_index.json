{
    "docs": [
        {
            "location": "/", 
            "text": "TFLearn: Deep learning library featuring a higher-level API for TensorFlow.\n\n\nTFlearn is a modular and transparent deep learning library built on top of Tensorflow.  It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it.\n\n\nTFLearn features include:\n\n\n\n\nEasy-to-use and understand high-level API for implementing deep neural networks, with tutorial and examples.\n\n\nFast prototyping through highly modular built-in neural network layers, regularizers, optimizers, metrics...\n\n\nFull transparency over Tensorflow. All functions are built over tensors and can be used independently of TFLearn.\n\n\nPowerful helpers functions to train any TensorFlow graph, with support of multiple inputs, outputs and optimizers.\n\n\nEasy and beautiful graph visualization, with details about weights, gradients, activations and more...\n\n\nEffortless device placement for using multiple CPU/GPU.\n\n\n\n\nThe high-level API currently supports most of recent deep learning models, such as Convolutions, LSTM, BiRNN, BatchNorm, PReLU, Residual networks, Generative networks... In the future, TFLearn is also intended to stay up-to-date with latest deep learning techniques.\n\n\nNote: This is the first release of TFLearn. Contributions are more than welcome!\n\n\nQuick overview\n\n\nCode Example\n\n\n# Classification\ntflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n\nnet = tflearn.input_data(shape=[None, 784])\nnet = tflearn.fully_connected(net, 64)\nnet = tflearn.dropout(net, 0.5)\nnet = tflearn.fully_connected(net, 10, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y)\n\n\n\n\n# Sequence Generation\nnet = tflearn.input_data(shape=[None, 100, 5000])\nnet = tflearn.lstm(net, 64)\nnet = tflearn.dropout(net, 0.5)\nnet = tflearn.fully_connected(net, 5000, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n\nmmodel = tflearn.SequenceGenerator(net, dictionary=idx, seq_maxlen=100)\nmodel.fit(X, Y)\nmodel.generate(50, temperature=1.0)\n\n\n\n\nThere are many more examples available \nhere\n.\n\n\nWhere to Start?\n\n\nTo install TFLearn, see: \nInstallation Guide\n.\n\n\nIf your version of Tensorflow is under 0.7: \nUpgrade Tensorflow\n.\n\n\nFor a tutorial: \nGetting Started with TFLearn\n.\n\n\nFor more examples: \nExamples List\n.\n\n\nTo browse the API, check the \nAPI Documentation\n.\n\n\nModel Visualization\n\n\nGraph\n\n\n\n\nLoss \n Accuracy (multiple runs)\n\n\n\n\nLayers\n\n\n\n\nSources\n\n\nGitHub: \nhttps://github.com/tflearn/tflearn\n.\n\n\nContributions\n\n\nThis is the first release of TFLearn, if you find any bug, please report it in the GitHub issues section.\n\n\nImprovements and requests for new features are more than welcome! Do not hesitate to twist and tweak TFLearn, and send pull-requests.\n\n\nFor more info: \nContribute to TFLearn\n.\n\n\nLicense\n\n\nMIT License", 
            "title": "Home"
        }, 
        {
            "location": "/#tflearn-deep-learning-library-featuring-a-higher-level-api-for-tensorflow", 
            "text": "TFlearn is a modular and transparent deep learning library built on top of Tensorflow.  It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it.  TFLearn features include:   Easy-to-use and understand high-level API for implementing deep neural networks, with tutorial and examples.  Fast prototyping through highly modular built-in neural network layers, regularizers, optimizers, metrics...  Full transparency over Tensorflow. All functions are built over tensors and can be used independently of TFLearn.  Powerful helpers functions to train any TensorFlow graph, with support of multiple inputs, outputs and optimizers.  Easy and beautiful graph visualization, with details about weights, gradients, activations and more...  Effortless device placement for using multiple CPU/GPU.   The high-level API currently supports most of recent deep learning models, such as Convolutions, LSTM, BiRNN, BatchNorm, PReLU, Residual networks, Generative networks... In the future, TFLearn is also intended to stay up-to-date with latest deep learning techniques.  Note: This is the first release of TFLearn. Contributions are more than welcome!", 
            "title": "TFLearn: Deep learning library featuring a higher-level API for TensorFlow."
        }, 
        {
            "location": "/#quick-overview", 
            "text": "Code Example  # Classification\ntflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n\nnet = tflearn.input_data(shape=[None, 784])\nnet = tflearn.fully_connected(net, 64)\nnet = tflearn.dropout(net, 0.5)\nnet = tflearn.fully_connected(net, 10, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y)  # Sequence Generation\nnet = tflearn.input_data(shape=[None, 100, 5000])\nnet = tflearn.lstm(net, 64)\nnet = tflearn.dropout(net, 0.5)\nnet = tflearn.fully_connected(net, 5000, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n\nmmodel = tflearn.SequenceGenerator(net, dictionary=idx, seq_maxlen=100)\nmodel.fit(X, Y)\nmodel.generate(50, temperature=1.0)  There are many more examples available  here .", 
            "title": "Quick overview"
        }, 
        {
            "location": "/#where-to-start", 
            "text": "To install TFLearn, see:  Installation Guide .  If your version of Tensorflow is under 0.7:  Upgrade Tensorflow .  For a tutorial:  Getting Started with TFLearn .  For more examples:  Examples List .  To browse the API, check the  API Documentation .", 
            "title": "Where to Start?"
        }, 
        {
            "location": "/#model-visualization", 
            "text": "Graph   Loss   Accuracy (multiple runs)   Layers", 
            "title": "Model Visualization"
        }, 
        {
            "location": "/#sources", 
            "text": "GitHub:  https://github.com/tflearn/tflearn .", 
            "title": "Sources"
        }, 
        {
            "location": "/#contributions", 
            "text": "This is the first release of TFLearn, if you find any bug, please report it in the GitHub issues section.  Improvements and requests for new features are more than welcome! Do not hesitate to twist and tweak TFLearn, and send pull-requests.  For more info:  Contribute to TFLearn .", 
            "title": "Contributions"
        }, 
        {
            "location": "/#license", 
            "text": "MIT License", 
            "title": "License"
        }, 
        {
            "location": "/doc_index/", 
            "text": "Documentation index\n\n\nBase\n\n\n\n\nIntroduction\n\n\nDocumentation Index\n\n\nTFLearn Installation\n\n\nGetting Started with TFLearn\n\n\nTFLearn Examples\n\n\n\n\nAPI\n\n\n\n\nModels\n\n\nDeep Neural Network\n\n\nGenerative Neural Network\n\n\n\n\n\n\nLayers\n\n\nCore Layers\n\n\nConvolutional Layers\n\n\nRecurrent Layers\n\n\nNormalization Layers\n\n\nEmbedding Layers\n\n\nMerge Layers\n\n\n\n\n\n\nBuilt-in Ops\n\n\nActivations\n\n\nObjectives\n\n\nOptimizers\n\n\nMetrics\n\n\nInitializations\n  \n\n\nLosses\n\n\nSummaries\n\n\n\n\n\n\nHelpers for extending Tensorflow\n\n\nTrainer\n\n\nPredictor\n\n\nGenerator\n\n\nSummarizer\n\n\nRegularizer\n\n\n\n\n\n\nOthers\n\n\nGraph Configuration\n\n\nData Utilities\n\n\n\n\n\n\n\n\nOthers\n\n\n\n\nContributions\n\n\nLicense", 
            "title": "Index"
        }, 
        {
            "location": "/doc_index/#documentation-index", 
            "text": "", 
            "title": "Documentation index"
        }, 
        {
            "location": "/doc_index/#base", 
            "text": "Introduction  Documentation Index  TFLearn Installation  Getting Started with TFLearn  TFLearn Examples", 
            "title": "Base"
        }, 
        {
            "location": "/doc_index/#api", 
            "text": "Models  Deep Neural Network  Generative Neural Network    Layers  Core Layers  Convolutional Layers  Recurrent Layers  Normalization Layers  Embedding Layers  Merge Layers    Built-in Ops  Activations  Objectives  Optimizers  Metrics  Initializations     Losses  Summaries    Helpers for extending Tensorflow  Trainer  Predictor  Generator  Summarizer  Regularizer    Others  Graph Configuration  Data Utilities", 
            "title": "API"
        }, 
        {
            "location": "/doc_index/#others", 
            "text": "Contributions  License", 
            "title": "Others"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\nTensorflow Installation\n\n\nTFLearn requires Tensorflow (version \n= 0.7.0) to be installed.\n\n\n# Ubuntu/Linux 64-bit, CPU only:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, GPU enabled:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n# Mac OS X, CPU only:\neasy_install --upgrade six\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n\n\n\n\n\n\nFor more details: \nTensorflow installation instructions\n.\n\n\n\n\nTFLearn Installation\n\n\nTo install TFLearn stable version, you can run:\n\n\npip install tflearn\n\n\n\n\nTo install the latest version of TFLearn, you can run:\n\n\npip install git+https://github.com/tflearn/tflearn.git\n\n\n\n\nOtherwise, you can also install from source by running (from source folder):\n\n\npython setup.py install\n\n\n\n\nUpgrade Tensorflow\n\n\nIf you version for Tensorflow is too old (under 0.7.0), you may upgrade Tensorflow to avoid some incompatibilities with TFLearn.\nTo upgrade Tensorflow, you first need to uninstall Tensorflow and Protobuf:\n\n\npip uninstall protobuf\npip uninstall tensorflow\n\n\n\n\nThen you can re-install Tensorflow:\n\n\n# Ubuntu/Linux 64-bit, CPU only:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, GPU enabled:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n# Mac OS X, CPU only:\neasy_install --upgrade six\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n\n\n\n\nUsing Latest Tensorflow\n\n\nTFLearn is compatible with \nmaster version\n of Tensorflow, but some warnings may appear.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#tensorflow-installation", 
            "text": "TFLearn requires Tensorflow (version  = 0.7.0) to be installed.  # Ubuntu/Linux 64-bit, CPU only:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, GPU enabled:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n# Mac OS X, CPU only:\neasy_install --upgrade six\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl   For more details:  Tensorflow installation instructions .", 
            "title": "Tensorflow Installation"
        }, 
        {
            "location": "/installation/#tflearn-installation", 
            "text": "To install TFLearn stable version, you can run:  pip install tflearn  To install the latest version of TFLearn, you can run:  pip install git+https://github.com/tflearn/tflearn.git  Otherwise, you can also install from source by running (from source folder):  python setup.py install", 
            "title": "TFLearn Installation"
        }, 
        {
            "location": "/installation/#upgrade-tensorflow", 
            "text": "If you version for Tensorflow is too old (under 0.7.0), you may upgrade Tensorflow to avoid some incompatibilities with TFLearn.\nTo upgrade Tensorflow, you first need to uninstall Tensorflow and Protobuf:  pip uninstall protobuf\npip uninstall tensorflow  Then you can re-install Tensorflow:  # Ubuntu/Linux 64-bit, CPU only:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, GPU enabled:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n# Mac OS X, CPU only:\neasy_install --upgrade six\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl", 
            "title": "Upgrade Tensorflow"
        }, 
        {
            "location": "/installation/#using-latest-tensorflow", 
            "text": "TFLearn is compatible with  master version  of Tensorflow, but some warnings may appear.", 
            "title": "Using Latest Tensorflow"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Getting started with TFLearn\n\n\nHere is a basic guide that introduces TFLearn and its functionalities. First highlighting TFLearn high-level API, for fast neural network building and training, and then showing how TFLearn layers, built-in ops and helpers can directly benefit any model implementation with Tensorflow.\n\n\nHigh-Level API usage\n\n\nTFLearn introduces a High-Level API that makes neural network building and training fast and easy. This API is intuitive and fully compatible with Tensorflow.\n\n\nLayers\n\n\nLayers are a core feature of TFLearn. While completely defining a model using Tensorflow ops can be time consuming and repetitive, TFLearn brings \"layers\" that represent an abstract set of operations to make building neural networks more convenient. For example, a convolutional layer will:\n\n\n\n\nCreate and initialize weights and biases variables\n\n\nApply convolution over incoming tensor\n\n\nAdd an activation function after the convolution\n\n\netc...\n\n\n\n\nIn Tensorflow, write those kind of operation set can be quite fastidious:\n\n\nwith tf.name_scope('conv1'):\n    W = tf.Variable(tf.random_normal([5, 5, 1, 32]), dtype=tf.float32, name='Weights')\n    b = tf.Variable(tf.random_normal([32]), dtype=tf.float32, name='biases')\n    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n    x = tf.add_bias(W, b)\n    x = tf.nn.relu(x)\n\n\n\n\nWhile in TFLearn, it only takes a line:\n\n\ntflearn.conv_2d(x, 32, 5, activation='relu', name='conv1')\n\n\n\n\nHere is a list of all currently available layers:\n\n\n\n\n\n\n\n\nFile\n\n\nLayers\n\n\n\n\n\n\n\n\n\n\ncore\n\n\ninput_data, fully_connected, dropout, custom_layer, reshape, flatten, activation, single_unit\n\n\n\n\n\n\nconv\n\n\nconv_2d, conv_2d_transpose, max_pool_2d, avg_pool_2d, conv_1d, max_pool_1d, avg_pool_1d, shallow_residual_block, deep_residual_block\n\n\n\n\n\n\nrecurrent\n\n\nsimple_rnn, lstm, gru, bidirectionnal_rnn, dynamic_rnn\n\n\n\n\n\n\nembedding\n\n\nembedding\n\n\n\n\n\n\nnormalization\n\n\nbatch_normalization, local_response_normalization\n\n\n\n\n\n\nmerge\n\n\nmerge, merge_outputs\n\n\n\n\n\n\nestimator\n\n\nregression\n\n\n\n\n\n\n\n\nBuilt-in Operations\n\n\nBesides layers concept, TFLearn also provides many different ops to be used while building a neural network. These ops are firstly mean to be used as part of the above 'layers' arguments, but they can also be used independently in any other Tensorflow graph for convenience.\n\n\n\n\n\n\n\n\nFile\n\n\nOps\n\n\n\n\n\n\n\n\n\n\nactivations\n\n\nlinear, tanh, sigmoid, softmax, softplus, softsign, relu, relu6, leaky_relu, prelu, elu\n\n\n\n\n\n\nobjectives\n\n\nsoftmax_categorical_crossentropy, categorical_crossentropy, binary_crossentropy, mean_square, hinge_loss\n\n\n\n\n\n\noptimizers\n\n\nSGD, RMSProp, Adam, Momentum, AdaGrad, Ftrl\n\n\n\n\n\n\nmetrics\n\n\nAccuracy, Top_k, R2\n\n\n\n\n\n\ninitializations\n\n\nzeros, uniform, uniform_scaling, normal, truncated_normal\n\n\n\n\n\n\nlosses\n\n\nl1, l2\n\n\n\n\n\n\n\n\nIn practice, the arguments (such as 'activation' or 'regularizer' of conv_2d) just require the op name. Below are some quick examples:\n\n\n# Activation and Regularization inside a layer:\nfc2 = tflearn.dense(fc1, 32, activation='tanh', regularizer='L2')\n# Equivalent to:\nfc2 = tflearn.dense(fc1, 32)\ntflearn.add_weights_regularization(fc2, loss='L2')\nfc2 = tflearn.relu(fc2)\n\n# Optimizer, Objective and Metric:\nreg = tflearn.regression(fc4, optimizer='rmsprop', metric='accuracy', loss='categorical_crossentropy')\n# Ops can also be defined outside, for deeper customization:\nmomentum = tflearn.optimizers.Momentum(learning_rate=0.1, weight_decay=0.96, decay_step=200)\ntop5 = tflearn.metrics.Top_k(k=5)\nreg = tflearn.regression(fc4, optimizer=momentum, metric=top5, loss='categorical_crossentropy')\n\n\n\n\nTraining, Evaluating \n Predicting\n\n\nTraining functions are another core feature of TFLearn. In Tensorflow, there are no pre-built API to train a network, so TFLearn integrates a set of functions to easily handle any neural network training, whatever the number of inputs, outputs and optimizers.\n\n\nIf you are using TFlearn layers, many parameters are already self managed, so it is very easy to train a model, using \nDNN\n class:\n\n\nnetwork = ... (some layers) ...\nnetwork = regression(network, optimizer='sgd', loss='categorical_crossentropy')\n\nmodel = DNN(network)\nmodel.fit(X, Y)\n\n\n\n\nIt can also directly be called for prediction, or evaluation:\n\n\nnetwork = ...\n\nmodel = DNN(network)\nmodel.load('model.tflearn')\nmodel.predict(X)\n\n\n\n\n\n\nTo learn more about those wrappers, see: \ndnn\n and \nestimator\n.\n\n\n\n\nVisualization\n\n\nWhile writing a Tensorflow model and adding tensorboard summaries isn't very practical, TFLearn has the ability to self managed a lot of useful logs. Currently, TFLearn training classes are supporting a verbose level to automatically manage summaries:\n\n\n\n\n0: Loss \n Metric (Best speed).\n\n\n1: Loss, Metric \n Gradients.\n\n\n2: Loss, Metric, Gradients \n Weights.\n\n\n3: Loss, Metric, Gradients, Weights, Activations \n Sparsity (Best Visualization).\n\n\n\n\nUsing \nDNN\n class, it is very simple, only specify the verbose level argument is required:\n\n\nmodel = DNN(network, tensorboard_verbose=3)\n\n\n\n\nThen, you can run Tensorboard and visualize your network and its performance:\n\n\n$ tensorboard --logdir='/tmp/tflearn_logs'\n\n\n\n\nGraph\n\n\n\n\nLoss \n Accuracy (multiple runs)\n\n\n\n\nLayers\n\n\n\n\nWeights persistence\n\n\nTo save or restore a model, you can simply invoke 'save' or 'load' method of \nDNN\n class.\n\n\n# Save a model\nmodel.save('my_model.tflearn')\n# Load a model\nmodel.load('my_model.tflearn')\n\n\n\n\nFine-tuning\n\n\nFine-tune a pre-trained model on a new task might be useful in many cases. So, when defining a model in TFLearn, you can specify which layer's weights you want to be restored or not (when loading pre-trained model). This can be handle with the 'restore' argument of layer functions (only available for layers with weights).\n\n\n# Weights will be restored by default.\ndense_layer = Dense(input_layer, 32)\n# Weights will not be restored, if specified so.\ndense_layer = Dense(input_layer, 32, restore='False')\n\n\n\n\nAll weights that doesn't need to be restored will be added to tf.GraphKeys.EXCL_RESTORE_VARS collection, and when loading a pre-trained model, these variables restoration will simply be ignored.\nThe following example shows how to fine-tune a network on a new task by restoring all weights except the last dense layer, and then train the new model on a new dataset:\n\n\n\n\nFine-tuning example: \nfinetuning.py\n.\n\n\n\n\nData management\n\n\nTFLearn supports numpy array data. Additionally, it also supports HDF5 for handling large datasets. HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data (\nmore info\n). TFLearn can directly use HDF5 formatted data:\n\n\n# Load hdf5 dataset\nh5f = h5py.File('data.h5', 'r')\nX, Y = h5f['MyLargeData']\n\n... define network ...\n\n# Use HDF5 data model to train model\nmodel = DNN(network)\nmodel.fit(X, Y)\n\n\n\n\nFor an example, see: \nhdf5.py\n.\n\n\nGraph Initialization\n\n\nIt might be useful to limit resources, or assigns more or less GPU RAM memory while training. To do so, a graph initializer can be used to configure a graph before run:\n\n\ntflearn.init_graph(set_seed=8888, num_cores=16, gpu_memory_fraction=0.5)\n\n\n\n\n\n\nSee: \nconfig\n.\n\n\n\n\nExtending Tensorflow\n\n\nTFLearn is a very flexible library designed to let you use any of its component independently. A model can be succinctly built using any combination of Tensorflow operations and TFLearn built-in layers and operations. The following instructions will show you the basics for extending Tensorflow with TFLearn.\n\n\nLayers\n\n\nAny layer can be used with any other Tensor from Tensorflow, this means that you can directly use TFLearn wrappers into your own Tensorflow graph.\n\n\n# Some operations using Tensorflow.\nX = tf.placeholder(shape=(None, 784), dtype=tf.float32)\nnet = tf.reshape(X, [-1, 28, 28, 1])\n\n# Using TFLearn convolution layer.\nnet = tflearn.conv_2d(net, 32, 3, activation='relu')\n\n# Using Tensorflow's max pooling op.\nnet = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n...\n\n\n\n\n\n\nFor an example, see: \nlayers.py\n.\n\n\n\n\nBuilt-in Operations\n\n\nTFLearn built-in ops makes Tensorflow graphs writing faster and more readable. So, similar to layers, built-in ops are fully compatible with any TensorFlow expression. The following code example shows how to use them along with pure Tensorflow API.\n\n\n\n\nSee: \nbuiltin_ops.py\n.\n\n\n\n\nHere is a list of available ops, click on the file for more details:\n\n\n\n\n\n\n\n\nFile\n\n\nOps\n\n\n\n\n\n\n\n\n\n\nactivations\n\n\nlinear, tanh, sigmoid, softmax, softplus, softsign, relu, relu6, leaky_relu, prelu, elu\n\n\n\n\n\n\nobjectives\n\n\nsoftmax_categorical_crossentropy, categorical_crossentropy, binary_crossentropy, mean_square, hinge_loss\n\n\n\n\n\n\noptimizers\n\n\nSGD, RMSProp, Adam, Momentum, AdaGrad, Ftrl\n\n\n\n\n\n\nmetrics\n\n\naccuracy_op, top_k_op, r2_op\n\n\n\n\n\n\ninitializations\n\n\nzeros, uniform, uniform_scaling, normal, truncated_normal\n\n\n\n\n\n\nlosses\n\n\nl1, l2\n\n\n\n\n\n\n\n\nNote:\n- Optimizers are designed as class and not function, for usage outside of TFlearn models, check: \noptimizers\n.\n\n\nTrainer / Evaluator / Predictor\n\n\nIf you are using you own Tensorflow model, TFLearn also provides some 'helpers' functions that can train any Tensorflow graph. It is suitable to make training more convenient, by introducing realtime monitoring, batch sampling, moving averages, tensorboard logs, data feeding, etc... It supports any number of inputs, outputs and optimization ops.\n\n\nTFLearn implements a \nTrainOp\n class to represent an optimization process (i.e. backprop). It is defined as follow:\n\n\ntrainop = TrainOp(net=my_network, loss=loss, metric=accuracy)\n\n\n\n\nThen, all TrainOp can be feeded into a \nTrainer\n class, that will handle the whole training process, considering all TrainOp together as a whole model.\n\n\nmodel = Trainer(trainops=trainop, tensorboard_dir='/tmp/tflearn')\nmodel.fit(feed_dict={input_placeholder: X, target_placeholder: Y})\n\n\n\n\nWhile most models will only have a single optimization process, it can be useful for more complex models to handle multiple ones.\n\n\nmodel = Trainer(trainops=[trainop1, trainop2])\nmodel.fit(feed_dict=[{in1: X1, label1: Y1}, {in2: X2, in3: X3, label2: Y2}])\n\n\n\n\n\n\n\n\nTo learn more about TrainOp and Trainer, see: \ntrainer\n.\n\n\n\n\n\n\nFor an example, see: \ntrainer.py\n.\n\n\n\n\n\n\nFor prediction, TFLearn implements a \nPredictor\n class that is working in a similar way as \nTrainer\n. It takes any network as parameter and return the predicted value.\n\n\nmodel = Predictor(network)\nmodel.predict(feed_dict={input_placeholder: X})\n\n\n\n\n\n\nTo learn more about Predictor class: \npredictor\n.\n\n\n\n\nTo handle network that behave differently at training and testing time (such as dropout and batch normalization), \nTrainer\n class uses a boolean variable ('training'), that specifies if the network is used for training or testing/predicting. This variable is stored under tf.GraphKeys.IS_TRAINING collection, as its first element.\nSo, when defining such ops, you need to add a condition to your op:\n\n\n# Example for Dropout:\nx = ...\n\ndef apply_dropout(): # Function to apply when training mode ON.\n  return tf.nn.dropout(x, keep_prob)\n\nis_training = tflearn.get_training_mode() # Retrieve is_training variable.\ntf.cond(is_training, apply_dropout, lambda: x) # Only apply dropout at training time.\n\n\n\n\nTo make it easy, TFLearn implements functions to retrieve that variable or change its value:\n\n\n\n\nSee: \ntraining config\n.\n\n\n\n\nVariables\n\n\nTFLearn defines a set of functions for users to quickly define variables.\n\n\nWhile in Tensorflow, variable creation requires predifinied value or initializer, as well as an explicit device placement, TFLearn simplify variable definition:\n\n\nimport tflearn.variables as vs\nmy_var = vs.variable('W',\n                     shape=[784, 128],\n                     initializer='truncated_normal',\n                     regularizer='L2',\n                     device='/gpu:0')\n\n\n\n\n\n\nFor an example, see: \nvariables.py\n.\n\n\n\n\nSummaries\n\n\nWhen using \nTrainer\n class, it is also very easy to manage summaries. It just additionally required that the activations to monitor are stored into \ntf.GraphKeys.ACTIVATIONS\n collection.\n\n\nThen, simply specify verbose level to control visualization depth:\n\n\nmodel = Trainer(network, loss=loss, metric=acc, tensorboard_verbose=3)\n\n\n\n\nBeside \nTrainer\n self-managed summaries option, you can also directly use TFLearn ops to quickly add summaries to your current Tensorflow graph.\n\n\nimport tflearn.helpers.summarizer as s\ns.summarize_variables(train_vars=[...]) # Summarize all given variables' weights (All trainable variables if None).\ns.summarize_activations(activations=[...]) # Summarize all given activations\ns.summarize_gradients(grads=[...]) # Summarize all given variables' gradient (All trainable variables if None).\ns.summarize(value, type) # Summarize anything.\n\n\n\n\nEvery function above accepts a collection as parameter, and will return a merged summary over that collection (Default name: 'tflearn_summ'). So you just need to run the last summarizer to get the whole summary ops collection, already merged.\n\n\ns.summarize_variables(collection='my_summaries')\ns.Summarize_gradients(collection='my_summaries')\nsummary_op = s.summarize_activations(collection='my_summaries')\n# summary_op is a the merged op of previously define weights, gradients and activations summary ops.\n\n\n\n\n\n\nFor an example, see: \nsummaries.py\n.\n\n\n\n\nRegularizers\n\n\nAdd regularization to a model can be completed using TFLearn \nregularizer\n helpers. It currently supports weights and activation regularization. Available regularization losses can be found in \nhere\n. All regularization losses are stored into tf.GraphKeys.REGULARIZATION_LOSSES collection.\n\n\n# Add L2 regularization to a variable\nW = tf.Variable(tf.random_normal([784, 256]), name=\nW\n)\ntflearn.add_weight_regularizer(W, 'L2', weight_decay=0.001)\n\n\n\n\nPreprocessing\n\n\nBesides tensor operations, it might be useful to perform some preprocessing on input data. Thus, TFLearn has a set of preprocessing functions to make data manipulation more convenient (such as sequence padding, categorical labels, shuffling at unison, image processing, etc...).\n\n\n\n\nFor more details, see: \ndata_utils\n.\n\n\n\n\nGetting Further\n\n\nThere are a lot of examples along with numerous neural network\nimplementations available for you to practice TFLearn more in depth:\n\n\n\n\nSee: \nExamples\n.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#getting-started-with-tflearn", 
            "text": "Here is a basic guide that introduces TFLearn and its functionalities. First highlighting TFLearn high-level API, for fast neural network building and training, and then showing how TFLearn layers, built-in ops and helpers can directly benefit any model implementation with Tensorflow.", 
            "title": "Getting started with TFLearn"
        }, 
        {
            "location": "/getting_started/#high-level-api-usage", 
            "text": "TFLearn introduces a High-Level API that makes neural network building and training fast and easy. This API is intuitive and fully compatible with Tensorflow.", 
            "title": "High-Level API usage"
        }, 
        {
            "location": "/getting_started/#layers", 
            "text": "Layers are a core feature of TFLearn. While completely defining a model using Tensorflow ops can be time consuming and repetitive, TFLearn brings \"layers\" that represent an abstract set of operations to make building neural networks more convenient. For example, a convolutional layer will:   Create and initialize weights and biases variables  Apply convolution over incoming tensor  Add an activation function after the convolution  etc...   In Tensorflow, write those kind of operation set can be quite fastidious:  with tf.name_scope('conv1'):\n    W = tf.Variable(tf.random_normal([5, 5, 1, 32]), dtype=tf.float32, name='Weights')\n    b = tf.Variable(tf.random_normal([32]), dtype=tf.float32, name='biases')\n    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n    x = tf.add_bias(W, b)\n    x = tf.nn.relu(x)  While in TFLearn, it only takes a line:  tflearn.conv_2d(x, 32, 5, activation='relu', name='conv1')  Here is a list of all currently available layers:     File  Layers      core  input_data, fully_connected, dropout, custom_layer, reshape, flatten, activation, single_unit    conv  conv_2d, conv_2d_transpose, max_pool_2d, avg_pool_2d, conv_1d, max_pool_1d, avg_pool_1d, shallow_residual_block, deep_residual_block    recurrent  simple_rnn, lstm, gru, bidirectionnal_rnn, dynamic_rnn    embedding  embedding    normalization  batch_normalization, local_response_normalization    merge  merge, merge_outputs    estimator  regression", 
            "title": "Layers"
        }, 
        {
            "location": "/getting_started/#built-in-operations", 
            "text": "Besides layers concept, TFLearn also provides many different ops to be used while building a neural network. These ops are firstly mean to be used as part of the above 'layers' arguments, but they can also be used independently in any other Tensorflow graph for convenience.     File  Ops      activations  linear, tanh, sigmoid, softmax, softplus, softsign, relu, relu6, leaky_relu, prelu, elu    objectives  softmax_categorical_crossentropy, categorical_crossentropy, binary_crossentropy, mean_square, hinge_loss    optimizers  SGD, RMSProp, Adam, Momentum, AdaGrad, Ftrl    metrics  Accuracy, Top_k, R2    initializations  zeros, uniform, uniform_scaling, normal, truncated_normal    losses  l1, l2     In practice, the arguments (such as 'activation' or 'regularizer' of conv_2d) just require the op name. Below are some quick examples:  # Activation and Regularization inside a layer:\nfc2 = tflearn.dense(fc1, 32, activation='tanh', regularizer='L2')\n# Equivalent to:\nfc2 = tflearn.dense(fc1, 32)\ntflearn.add_weights_regularization(fc2, loss='L2')\nfc2 = tflearn.relu(fc2)\n\n# Optimizer, Objective and Metric:\nreg = tflearn.regression(fc4, optimizer='rmsprop', metric='accuracy', loss='categorical_crossentropy')\n# Ops can also be defined outside, for deeper customization:\nmomentum = tflearn.optimizers.Momentum(learning_rate=0.1, weight_decay=0.96, decay_step=200)\ntop5 = tflearn.metrics.Top_k(k=5)\nreg = tflearn.regression(fc4, optimizer=momentum, metric=top5, loss='categorical_crossentropy')", 
            "title": "Built-in Operations"
        }, 
        {
            "location": "/getting_started/#training-evaluating-predicting", 
            "text": "Training functions are another core feature of TFLearn. In Tensorflow, there are no pre-built API to train a network, so TFLearn integrates a set of functions to easily handle any neural network training, whatever the number of inputs, outputs and optimizers.  If you are using TFlearn layers, many parameters are already self managed, so it is very easy to train a model, using  DNN  class:  network = ... (some layers) ...\nnetwork = regression(network, optimizer='sgd', loss='categorical_crossentropy')\n\nmodel = DNN(network)\nmodel.fit(X, Y)  It can also directly be called for prediction, or evaluation:  network = ...\n\nmodel = DNN(network)\nmodel.load('model.tflearn')\nmodel.predict(X)   To learn more about those wrappers, see:  dnn  and  estimator .", 
            "title": "Training, Evaluating &amp; Predicting"
        }, 
        {
            "location": "/getting_started/#visualization", 
            "text": "While writing a Tensorflow model and adding tensorboard summaries isn't very practical, TFLearn has the ability to self managed a lot of useful logs. Currently, TFLearn training classes are supporting a verbose level to automatically manage summaries:   0: Loss   Metric (Best speed).  1: Loss, Metric   Gradients.  2: Loss, Metric, Gradients   Weights.  3: Loss, Metric, Gradients, Weights, Activations   Sparsity (Best Visualization).   Using  DNN  class, it is very simple, only specify the verbose level argument is required:  model = DNN(network, tensorboard_verbose=3)  Then, you can run Tensorboard and visualize your network and its performance:  $ tensorboard --logdir='/tmp/tflearn_logs'  Graph   Loss   Accuracy (multiple runs)   Layers", 
            "title": "Visualization"
        }, 
        {
            "location": "/getting_started/#weights-persistence", 
            "text": "To save or restore a model, you can simply invoke 'save' or 'load' method of  DNN  class.  # Save a model\nmodel.save('my_model.tflearn')\n# Load a model\nmodel.load('my_model.tflearn')", 
            "title": "Weights persistence"
        }, 
        {
            "location": "/getting_started/#fine-tuning", 
            "text": "Fine-tune a pre-trained model on a new task might be useful in many cases. So, when defining a model in TFLearn, you can specify which layer's weights you want to be restored or not (when loading pre-trained model). This can be handle with the 'restore' argument of layer functions (only available for layers with weights).  # Weights will be restored by default.\ndense_layer = Dense(input_layer, 32)\n# Weights will not be restored, if specified so.\ndense_layer = Dense(input_layer, 32, restore='False')  All weights that doesn't need to be restored will be added to tf.GraphKeys.EXCL_RESTORE_VARS collection, and when loading a pre-trained model, these variables restoration will simply be ignored.\nThe following example shows how to fine-tune a network on a new task by restoring all weights except the last dense layer, and then train the new model on a new dataset:   Fine-tuning example:  finetuning.py .", 
            "title": "Fine-tuning"
        }, 
        {
            "location": "/getting_started/#data-management", 
            "text": "TFLearn supports numpy array data. Additionally, it also supports HDF5 for handling large datasets. HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data ( more info ). TFLearn can directly use HDF5 formatted data:  # Load hdf5 dataset\nh5f = h5py.File('data.h5', 'r')\nX, Y = h5f['MyLargeData']\n\n... define network ...\n\n# Use HDF5 data model to train model\nmodel = DNN(network)\nmodel.fit(X, Y)  For an example, see:  hdf5.py .", 
            "title": "Data management"
        }, 
        {
            "location": "/getting_started/#graph-initialization", 
            "text": "It might be useful to limit resources, or assigns more or less GPU RAM memory while training. To do so, a graph initializer can be used to configure a graph before run:  tflearn.init_graph(set_seed=8888, num_cores=16, gpu_memory_fraction=0.5)   See:  config .", 
            "title": "Graph Initialization"
        }, 
        {
            "location": "/getting_started/#extending-tensorflow", 
            "text": "TFLearn is a very flexible library designed to let you use any of its component independently. A model can be succinctly built using any combination of Tensorflow operations and TFLearn built-in layers and operations. The following instructions will show you the basics for extending Tensorflow with TFLearn.", 
            "title": "Extending Tensorflow"
        }, 
        {
            "location": "/getting_started/#layers_1", 
            "text": "Any layer can be used with any other Tensor from Tensorflow, this means that you can directly use TFLearn wrappers into your own Tensorflow graph.  # Some operations using Tensorflow.\nX = tf.placeholder(shape=(None, 784), dtype=tf.float32)\nnet = tf.reshape(X, [-1, 28, 28, 1])\n\n# Using TFLearn convolution layer.\nnet = tflearn.conv_2d(net, 32, 3, activation='relu')\n\n# Using Tensorflow's max pooling op.\nnet = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n...   For an example, see:  layers.py .", 
            "title": "Layers"
        }, 
        {
            "location": "/getting_started/#built-in-operations_1", 
            "text": "TFLearn built-in ops makes Tensorflow graphs writing faster and more readable. So, similar to layers, built-in ops are fully compatible with any TensorFlow expression. The following code example shows how to use them along with pure Tensorflow API.   See:  builtin_ops.py .   Here is a list of available ops, click on the file for more details:     File  Ops      activations  linear, tanh, sigmoid, softmax, softplus, softsign, relu, relu6, leaky_relu, prelu, elu    objectives  softmax_categorical_crossentropy, categorical_crossentropy, binary_crossentropy, mean_square, hinge_loss    optimizers  SGD, RMSProp, Adam, Momentum, AdaGrad, Ftrl    metrics  accuracy_op, top_k_op, r2_op    initializations  zeros, uniform, uniform_scaling, normal, truncated_normal    losses  l1, l2     Note:\n- Optimizers are designed as class and not function, for usage outside of TFlearn models, check:  optimizers .", 
            "title": "Built-in Operations"
        }, 
        {
            "location": "/getting_started/#trainer-evaluator-predictor", 
            "text": "If you are using you own Tensorflow model, TFLearn also provides some 'helpers' functions that can train any Tensorflow graph. It is suitable to make training more convenient, by introducing realtime monitoring, batch sampling, moving averages, tensorboard logs, data feeding, etc... It supports any number of inputs, outputs and optimization ops.  TFLearn implements a  TrainOp  class to represent an optimization process (i.e. backprop). It is defined as follow:  trainop = TrainOp(net=my_network, loss=loss, metric=accuracy)  Then, all TrainOp can be feeded into a  Trainer  class, that will handle the whole training process, considering all TrainOp together as a whole model.  model = Trainer(trainops=trainop, tensorboard_dir='/tmp/tflearn')\nmodel.fit(feed_dict={input_placeholder: X, target_placeholder: Y})  While most models will only have a single optimization process, it can be useful for more complex models to handle multiple ones.  model = Trainer(trainops=[trainop1, trainop2])\nmodel.fit(feed_dict=[{in1: X1, label1: Y1}, {in2: X2, in3: X3, label2: Y2}])    To learn more about TrainOp and Trainer, see:  trainer .    For an example, see:  trainer.py .    For prediction, TFLearn implements a  Predictor  class that is working in a similar way as  Trainer . It takes any network as parameter and return the predicted value.  model = Predictor(network)\nmodel.predict(feed_dict={input_placeholder: X})   To learn more about Predictor class:  predictor .   To handle network that behave differently at training and testing time (such as dropout and batch normalization),  Trainer  class uses a boolean variable ('training'), that specifies if the network is used for training or testing/predicting. This variable is stored under tf.GraphKeys.IS_TRAINING collection, as its first element.\nSo, when defining such ops, you need to add a condition to your op:  # Example for Dropout:\nx = ...\n\ndef apply_dropout(): # Function to apply when training mode ON.\n  return tf.nn.dropout(x, keep_prob)\n\nis_training = tflearn.get_training_mode() # Retrieve is_training variable.\ntf.cond(is_training, apply_dropout, lambda: x) # Only apply dropout at training time.  To make it easy, TFLearn implements functions to retrieve that variable or change its value:   See:  training config .", 
            "title": "Trainer / Evaluator / Predictor"
        }, 
        {
            "location": "/getting_started/#variables", 
            "text": "TFLearn defines a set of functions for users to quickly define variables.  While in Tensorflow, variable creation requires predifinied value or initializer, as well as an explicit device placement, TFLearn simplify variable definition:  import tflearn.variables as vs\nmy_var = vs.variable('W',\n                     shape=[784, 128],\n                     initializer='truncated_normal',\n                     regularizer='L2',\n                     device='/gpu:0')   For an example, see:  variables.py .", 
            "title": "Variables"
        }, 
        {
            "location": "/getting_started/#summaries", 
            "text": "When using  Trainer  class, it is also very easy to manage summaries. It just additionally required that the activations to monitor are stored into  tf.GraphKeys.ACTIVATIONS  collection.  Then, simply specify verbose level to control visualization depth:  model = Trainer(network, loss=loss, metric=acc, tensorboard_verbose=3)  Beside  Trainer  self-managed summaries option, you can also directly use TFLearn ops to quickly add summaries to your current Tensorflow graph.  import tflearn.helpers.summarizer as s\ns.summarize_variables(train_vars=[...]) # Summarize all given variables' weights (All trainable variables if None).\ns.summarize_activations(activations=[...]) # Summarize all given activations\ns.summarize_gradients(grads=[...]) # Summarize all given variables' gradient (All trainable variables if None).\ns.summarize(value, type) # Summarize anything.  Every function above accepts a collection as parameter, and will return a merged summary over that collection (Default name: 'tflearn_summ'). So you just need to run the last summarizer to get the whole summary ops collection, already merged.  s.summarize_variables(collection='my_summaries')\ns.Summarize_gradients(collection='my_summaries')\nsummary_op = s.summarize_activations(collection='my_summaries')\n# summary_op is a the merged op of previously define weights, gradients and activations summary ops.   For an example, see:  summaries.py .", 
            "title": "Summaries"
        }, 
        {
            "location": "/getting_started/#regularizers", 
            "text": "Add regularization to a model can be completed using TFLearn  regularizer  helpers. It currently supports weights and activation regularization. Available regularization losses can be found in  here . All regularization losses are stored into tf.GraphKeys.REGULARIZATION_LOSSES collection.  # Add L2 regularization to a variable\nW = tf.Variable(tf.random_normal([784, 256]), name= W )\ntflearn.add_weight_regularizer(W, 'L2', weight_decay=0.001)", 
            "title": "Regularizers"
        }, 
        {
            "location": "/getting_started/#preprocessing", 
            "text": "Besides tensor operations, it might be useful to perform some preprocessing on input data. Thus, TFLearn has a set of preprocessing functions to make data manipulation more convenient (such as sequence padding, categorical labels, shuffling at unison, image processing, etc...).   For more details, see:  data_utils .", 
            "title": "Preprocessing"
        }, 
        {
            "location": "/getting_started/#getting-further", 
            "text": "There are a lot of examples along with numerous neural network\nimplementations available for you to practice TFLearn more in depth:   See:  Examples .", 
            "title": "Getting Further"
        }, 
        {
            "location": "/examples/", 
            "text": "TFLearn Examples\n\n\nBasics\n\n\n\n\nLinear Regression\n. Implement a linear regression using TFLearn.\n\n\nLogical Operators\n. Implement logical operators with TFLearn (also includes a usage of 'merge').\n\n\nWeights Persistence\n. Save and Restore a model.\n\n\nFine-Tuning\n. Fine-Tune a pre-trained model on a new task.\n\n\nUsing HDF5\n. Use HDF5 to handle large datasets.\n\n\nUsing DASK\n. Use DASK to handle large datasets.\n\n\n\n\nExtending Tensorflow\n\n\n\n\nLayers\n. Use TFLearn layers along with Tensorflow.\n\n\nTrainer\n. Use TFLearn trainer class to train any Tensorflow graph.\n\n\nBuilt-in Ops\n. Use TFLearn built-in operations along with Tensorflow.\n\n\nSummaries\n. Use TFLearn summarizers along with Tensorflow.\n\n\nVariables\n. Use TFLearn variables along with Tensorflow.\n\n\n\n\nComputer Vision\n\n\n\n\nMulti-layer perceptron\n. A multi-layer perceptron implementation for MNIST classification task.\n\n\nConvolutional Network (MNIST)\n. A Convolutional neural network implementation for classifying MNIST dataset.\n\n\nConvolutional Network (CIFAR-10)\n. A Convolutional neural network implementation for classifying CIFAR-10 dataset.\n\n\nNetwork in Network\n. 'Network in Network' implementation for classifying CIFAR-10 dataset.\n\n\nAlexnet\n. Apply Alexnet to Oxford Flowers 17 classification task.\n\n\nVGGNet\n. Apply VGG Network to Oxford Flowers 17 classification task.\n\n\nRNN Pixels\n. Use RNN (over sequence of pixels) to classify images.\n\n\nResidual Network (MNIST)\n. A residual network with shallow bottlenecks applied to MNIST classification task.\n\n\nResidual Network (CIFAR-10)\n. A residual network with deep bottlenecks applied to CIFAR-10 classification task.\n\n\n\n\nNatural Language Processing\n\n\n\n\nReccurent Network (LSTM)\n. Apply an LSTM to IMDB sentiment dataset classification task.\n\n\nBi-Directional LSTM\n. Apply a bi-directional LSTM to IMDB sentiment dataset classification task.\n\n\nCity Name Generation\n. Generates new US-cities name, using LSTM network.\n\n\nShakespeare Scripts Generation\n. Generates new Shakespeare scripts, using LSTM network.", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#tflearn-examples", 
            "text": "", 
            "title": "TFLearn Examples"
        }, 
        {
            "location": "/examples/#basics", 
            "text": "Linear Regression . Implement a linear regression using TFLearn.  Logical Operators . Implement logical operators with TFLearn (also includes a usage of 'merge').  Weights Persistence . Save and Restore a model.  Fine-Tuning . Fine-Tune a pre-trained model on a new task.  Using HDF5 . Use HDF5 to handle large datasets.  Using DASK . Use DASK to handle large datasets.", 
            "title": "Basics"
        }, 
        {
            "location": "/examples/#extending-tensorflow", 
            "text": "Layers . Use TFLearn layers along with Tensorflow.  Trainer . Use TFLearn trainer class to train any Tensorflow graph.  Built-in Ops . Use TFLearn built-in operations along with Tensorflow.  Summaries . Use TFLearn summarizers along with Tensorflow.  Variables . Use TFLearn variables along with Tensorflow.", 
            "title": "Extending Tensorflow"
        }, 
        {
            "location": "/examples/#computer-vision", 
            "text": "Multi-layer perceptron . A multi-layer perceptron implementation for MNIST classification task.  Convolutional Network (MNIST) . A Convolutional neural network implementation for classifying MNIST dataset.  Convolutional Network (CIFAR-10) . A Convolutional neural network implementation for classifying CIFAR-10 dataset.  Network in Network . 'Network in Network' implementation for classifying CIFAR-10 dataset.  Alexnet . Apply Alexnet to Oxford Flowers 17 classification task.  VGGNet . Apply VGG Network to Oxford Flowers 17 classification task.  RNN Pixels . Use RNN (over sequence of pixels) to classify images.  Residual Network (MNIST) . A residual network with shallow bottlenecks applied to MNIST classification task.  Residual Network (CIFAR-10) . A residual network with deep bottlenecks applied to CIFAR-10 classification task.", 
            "title": "Computer Vision"
        }, 
        {
            "location": "/examples/#natural-language-processing", 
            "text": "Reccurent Network (LSTM) . Apply an LSTM to IMDB sentiment dataset classification task.  Bi-Directional LSTM . Apply a bi-directional LSTM to IMDB sentiment dataset classification task.  City Name Generation . Generates new US-cities name, using LSTM network.  Shakespeare Scripts Generation . Generates new Shakespeare scripts, using LSTM network.", 
            "title": "Natural Language Processing"
        }, 
        {
            "location": "/models/dnn/", 
            "text": "Deep Neural Network Model\n\n\ntflearn.models.dnn.DNN\n  (network,  clip_gradients=5.0,  tensorboard_verbose=0,  tensorboard_dir='/tmp/tflearn_logs/',  checkpoint_path=None,  max_checkpoints=None)\n\n\nArguments\n\n\n\n\n\nnetwork\n: \nTensor\n. Neural network to be used.\n\n\ntensorboard_verbose\n: \nint\n. Summary verbose level, it accepts\ndifferent levels of tensorboard logs:\n\n\n\n\n0: Loss, Accuracy (Best Speed).\n1: Loss, Accuracy, Gradients.\n2: Loss, Accuracy, Gradients, Weights.\n3: Loss, Accuracy, Gradients, Weights, Activations, Sparsity.(Best visualization)\n\n\n\n\n\n\ntensorboard_dir\n: \nstr\n. Directory to store tensorboard logs.\nDefault: \"/tmp/tflearn_logs/\"\n\n\ncheckpoint_path\n: \nstr\n. Path to store model checkpoints. If None,\nno model checkpoint will be saved. Default: None.\n\n\nmax_checkpoints\n: \nint\n or None. Maximum amount of checkpoints. If\nNone, no limit. Default: None.\n\n\n\n\nAttributes\n\n\n\n\n\ntrainer\n: \nTrainer\n. Handle model training.\n\n\npredictor\n: \nPredictor\n. Handle model prediction.\n\n\n\n\nMethods\n\n\n\n \n\n\nevaluate\n  (X,  Y,  batch_size)\n\n\nEvaluate model on given samples.\n\n\nArguments\n\n\n\n\n\nX\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with inputs layer name as keys). Data to feed to train\nmodel.\n\n\nY\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with estimators layer name as keys). Targets (Labels) to\nfeed to train model. Usually set as the next element of a\nsequence, i.e. for x[0] =\n y[0] = x[1].\n\n\nbatch_size\n: \nint\n. The batch size. Default: 128.\n\n\n\n\nReturns\n\n\n\nThe metric score.\n\n\n \n\n\nfit\n  (X_inputs,  Y_targets,  n_epoch=10,  validation_set=None,  show_metric=False,  batch_size=None,  shuffle=None,  snapshot_epoch=True,  snapshot_step=None,  run_id=None)\n\n\nTrain model, feeding X_inputs and Y_targets to the network.\n\n\nNOTE: When not feeding dicts, data assignations is made by\ninput/estimator layers creation order (For example, the second\ninput layer created will be feeded by the second value of\nX_inputs list).\n\n\nExamples\n\n\n\nmodel.fit(X, Y) # Single input and output\nmodel.fit({'input1': X}, {'output1': Y}) # Single input and output\nmodel.fit([X1, X2], Y) # Mutliple inputs, Single output\n\n# validate with X_val and [Y1_val, Y2_val]\nmodel.fit(X, [Y1, Y2], validation_set=(X_val, [Y1_val, Y2_val]))\n# 10% of training data used for validation\nmodel.fit(X, Y, validation_set=0.1)\n\n\n\n\nArguments\n\n\n\n\n\nX_inputs\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with inputs layer name as keys). Data to feed to train\nmodel.\n\n\nY_targets\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with estimators layer name as keys). Targets (Labels) to\nfeed to train model.\n\n\nn_epoch\n: \nint\n. Number of epoch to run. Default: None.\n\n\nvalidation_set\n: \ntuple\n. Represents data used for validation.\n\ntuple\n holds data and targets (provided as same type as\nX_inputs and Y_targets). Additionally, it also accepts\n\nfloat\n (\n1) to performs a data split over training data.\n\n\nshow_metric\n: \nbool\n. Display or not accuracy at every step.\n\n\nbatch_size\n: \nint\n or None. If \nint\n, overrides all network\nestimators 'batch_size' by this value.\n\n\nshuffle\n: \nbool\n or None. If \nbool\n, overrides all network\nestimators 'shuffle' by this value.\n\n\nsnapshot_epoch\n: \nbool\n. If True, it will snapshot model at the end\nof every epoch. (Snapshot a model will evaluate this model\non validation set, as well as create a checkpoint if\n'checkpoint_path' specified).\n\n\nsnapshot_step\n: \nint\n or None. If \nint\n, it will snapshot model\nevery 'snapshot_step' steps.\n\n\nrun_id\n: \nstr\n. Give a name for this run. (Useful for Tensorboard).\n\n\n\n\n \n\n\nget_weights\n  (weight_tensor)\n\n\nGet a variable weights.\n\n\nExamples\n\n\n\ndnn = DNNTrainer(...)\nw = dnn.get_weights(denselayer.W) # get a dense layer weights\nw = dnn.get_weights(convlayer.b) # get a conv layer biases\n\n\n\n\nArguments\n\n\n\n\n\nweight_tensor\n: \nTensor\n. A Variable.\n\n\n\n\nReturns\n\n\n\nnp.array\n. The provided variable weights.\n\n\n \n\n\nload\n  (model_file)\n\n\nRestore model weights.\n\n\nArguments\n\n\n\n\n\nmodel_file\n: \nstr\n. Model path.\n\n\n\n\n \n\n\npredict\n  (X)\n\n\nModel prediction for given input data.\n\n\nArguments\n\n\n\n\n\nX\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with inputs layer name as keys). Data to feed for prediction.\n\n\n\n\nReturns\n\n\n\narray or \nlist\n of array. The predicted value.\n\n\n \n\n\nsave\n  (model_file)\n\n\nSave model weights.\n\n\nArguments\n\n\n\n\n\nmodel_file\n: \nstr\n. Model path.\n\n\n\n\n \n\n\nset_weights\n  (tensor,  weights)\n\n\nAssign a tensor variable a given value.\n\n\nArguments\n\n\n\n\n\ntensor\n: \nTensor\n. The tensor variable to assign value.\n\n\nweights\n: The value to be assigned.", 
            "title": "Deep Neural Network"
        }, 
        {
            "location": "/models/dnn/#deep-neural-network-model", 
            "text": "tflearn.models.dnn.DNN   (network,  clip_gradients=5.0,  tensorboard_verbose=0,  tensorboard_dir='/tmp/tflearn_logs/',  checkpoint_path=None,  max_checkpoints=None)", 
            "title": "Deep Neural Network Model"
        }, 
        {
            "location": "/models/generator/", 
            "text": "Sequence Generator Model\n\n\ntflearn.models.generator.SequenceGenerator\n  (network,  dictionary=None,  seq_maxlen=25,  clip_gradients=0.0,  tensorboard_verbose=0,  tensorboard_dir='/tmp/tflearn_logs/',  checkpoint_path=None,  max_checkpoints=None)\n\n\nA deep neural network model for generating sequences.\n\n\nArguments\n\n\n\n\n\nnetwork\n: \nTensor\n. Neural network to be used.\n\n\ndictionary\n: \ndict\n. A dictionary associating each sample with a key (\nusually integers). For example: {'a': 0, 'b': 1, 'c': 2, ...}.\n\n\nseq_maxlen\n: \nint\n. The maximum length of a sequence.\n\n\ntensorboard_verbose\n: \nint\n. Summary verbose level, it accepts\ndifferent levels of tensorboard logs:\n\n\n\n\n0 - Loss, Accuracy (Best Speed).\n1 - Loss, Accuracy, Gradients.\n2 - Loss, Accuracy, Gradients, Weights.\n3 - Loss, Accuracy, Gradients, Weights, Activations, Sparsity.(Best visualization)\n\n\n\n\n\n\ntensorboard_dir\n: \nstr\n. Directory to store tensorboard logs.\nDefault: \"/tmp/tflearn_logs/\"\n\n\ncheckpoint_path\n: \nstr\n. Path to store model checkpoints. If None,\nno model checkpoint will be saved. Default: None.\n\n\nmax_checkpoints\n: \nint\n or None. Maximum amount of checkpoints. If\nNone, no limit. Default: None.\n\n\n\n\nAttributes\n\n\n\n\n\ntrainer\n: \nTrainer\n. Handle model training.\n\n\npredictor\n: \nPredictor\n. Handle model prediction.\n\n\n\n\nMethods\n\n\n\n \n\n\nevaluate\n  (X,  Y,  batch_size=128)\n\n\nEvaluate model on given samples.\n\n\nArguments\n\n\n\n\n\nX\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with inputs layer name as keys). Data to feed to train\nmodel.\n\n\nY\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with estimators layer name as keys). Targets (Labels) to\nfeed to train model. Usually set as the next element of a\nsequence, i.e. for x[0] =\n y[0] = x[1].\n\n\nbatch_size\n: \nint\n. The batch size. Default: 128.\n\n\n\n\nReturns\n\n\n\nThe metric score.\n\n\n \n\n\nfit\n  (X_inputs,  Y_targets,  n_epoch=10,  validation_set=None,  show_metric=False,  batch_size=None,  shuffle=None,  snapshot_epoch=True,  snapshot_step=None,  run_id=None)\n\n\nTrain model, feeding X_inputs and Y_targets to the network.\n\n\nNOTE: When not feeding dicts, data assignations is made by\ninput/estimator layers creation order (For example, the second\ninput layer created will be feeded by the second value of\nX_inputs list).\n\n\nExamples\n\n\n\nmodel.fit(X, Y) # Single input and output\nmodel.fit({'input1': X}, {'output1': Y}) # Single input and output\nmodel.fit([X1, X2], Y) # Mutliple inputs, Single output\n\n# validate with X_val and [Y1_val, Y2_val]\nmodel.fit(X, [Y1, Y2], validation_set=(X_val, [Y1_val, Y2_val]))\n# 10% of training data used for validation\nmodel.fit(X, Y, validation_set=0.1)\n\n\n\n\nArguments\n\n\n\n\n\nX_inputs\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with inputs layer name as keys). Data to feed to train\nmodel.\n\n\nY_targets\n: array, \nlist\n of array (if multiple inputs) or \ndict\n\n(with estimators layer name as keys). Targets (Labels) to\nfeed to train model. Usually set as the next element of a\nsequence, i.e. for x[0] =\n y[0] = x[1].\n\n\nn_epoch\n: \nint\n. Number of epoch to run. Default: None.\n\n\nvalidation_set\n: \ntuple\n. Represents data used for validation.\n\ntuple\n holds data and targets (provided as same type as\nX_inputs and Y_targets). Additionally, it also accepts\n\nfloat\n (\n1) to performs a data split over training data.\n\n\nshow_metric\n: \nbool\n. Display or not accuracy at every step.\n\n\nbatch_size\n: \nint\n or None. If \nint\n, overrides all network\nestimators 'batch_size' by this value.\n\n\nshuffle\n: \nbool\n or None. If \nbool\n, overrides all network\nestimators 'shuffle' by this value.\n\n\nsnapshot_epoch\n: \nbool\n. If True, it will snapshot model at the end\nof every epoch. (Snapshot a model will evaluate this model\non validation set, as well as create a checkpoint if\n'checkpoint_path' specified).\n\n\nsnapshot_step\n: \nint\n or None. If \nint\n, it will snapshot model\nevery 'snapshot_step' steps.\n\n\nrun_id\n: \nstr\n. Give a name for this run. (Useful for Tensorboard).\n\n\n\n\n \n\n\ngenerate\n  (seq_length,  temperature=0.5,  seq_seed=None,  display=False)\n\n\nGenerate a sequence. Temperature is controlling the novelty of\nthe created sequence, a temperature near 0 will looks like samples\nused for training, while the higher the temperature, the more novelty.\nFor optimal results, it is suggested to set sequence seed as some\nrandom sequence samples from training dataset.\n\n\nArguments\n\n\n\n\n\nseq_length\n: \nint\n. The generated sequence length.\n\n\ntemperature\n: \nfloat\n. Novelty rate.\n\n\nseq_seed\n: \nsequence\n. A sequence used as a seed to generate a\nnew sequence. Suggested to be a sequence from data used for\ntraining.\n\n\ndisplay\n: \nbool\n. If True, print sequence as it is generated.\n\n\n\n\nReturns\n\n\n\nThe generated sequence.\n\n\n \n\n\nget_weights\n  (weight_tensor)\n\n\nGet a variable weights.\n\n\nExamples\n\n\n\nsgen = SequenceGenerator(...)\nw = sgen.get_weights(denselayer.W) -- get a dense layer weights\n\n\nArguments\n\n\n\n\n\nweight_tensor\n: \ntf.Tensor\n. A Variable.\n\n\n\n\nReturns\n\n\n\nnp.array\n. The provided variable weights.\n\n\n \n\n\nload\n  (model_file)\n\n\nRestore model weights.\n\n\nArguments\n\n\n\n\n\nmodel_file\n: \nstr\n. Model path.\n\n\n\n\n \n\n\nsave\n  (model_file)\n\n\nSave model weights.\n\n\nArguments\n\n\n\n\n\nmodel_file\n: \nstr\n. Model path.\n\n\n\n\n \n\n\nset_weights\n  (tensor,  weights)\n\n\nAssign a tensor variable a given value.\n\n\nArguments\n\n\n\n\n\ntensor\n: \nTensor\n. The tensor variable to assign value.\n\n\nweights\n: The value to be assigned.", 
            "title": "Generative Neural Network"
        }, 
        {
            "location": "/models/generator/#sequence-generator-model", 
            "text": "tflearn.models.generator.SequenceGenerator   (network,  dictionary=None,  seq_maxlen=25,  clip_gradients=0.0,  tensorboard_verbose=0,  tensorboard_dir='/tmp/tflearn_logs/',  checkpoint_path=None,  max_checkpoints=None)  A deep neural network model for generating sequences.", 
            "title": "Sequence Generator Model"
        }, 
        {
            "location": "/layers/core/", 
            "text": "Input Data\n\n\ntflearn.layers.core.input_data\n  (shape=None,  placeholder=None,  dtype=tf.float32,  name='InputData')\n\n\ninput_data\n is used as a data entry (placeholder) of a network.\nThis placeholder will be feeded with data when training\n\n\nThis layer is used to keep track of the network inputs, by adding the\nplaceholder to INPUTS graphkey. TFLearn training functions may retrieve\nthose variables to setup the network training process.\n\n\nInput\n\n\n\nList of \nint\n (Shape), to create a new placeholder.\nOr\n\nTensor\n (Placeholder), to use an existing placeholder.\n\n\nOutput\n\n\n\nPlaceholder Tensor with given shape.\n\n\nArguments\n\n\n\n\n\nshape\n: list of \nint\n. An array or tuple representing input data shape.\nIt is required if no placeholder provided. First element should\nbe 'None' (representing batch size), if not provided, it will be\nadded automatically.\n\n\nplaceholder\n: A Placeholder to use for feeding this layer (optional).\nIf not specified, a placeholder will be automatically created.\nYou can retrieve that placeholder through graph key: 'INPUTS',\nor the 'placeholder' attribute of this function's returned tensor.\n\n\ndtype\n: \ntf.type\n, Placeholder data type (optional). Default: float32.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\nFully Connected\n\n\ntflearn.layers.core.fully_connected\n  (incoming,  n_units,  activation='linear',  bias=True,  weights_init='truncated_normal',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='Dense')\n\n\nA fully connected layer.\n\n\nInput\n\n\n\n(2+)-D Tensor [samples, input dim]. If not 2D, input will be flatten.\n\n\nOutput\n\n\n\n2D Tensor [samples, n_units].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming (2+)D Tensor.\n\n\nn_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(see tflearn.initializations) Default: 'truncated_normal'.\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n(see tflearn.initializations) Default: 'zeros'.\n   regularizer: \nstr\n (name) or \nTensor\n. Add a regularizer to this\nlayer weights (see tflearn.regularizers). Default: None.\n   weight_decay: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n   trainable: \nbool\n. If True, weights will be trainable.\n   restore: \nbool\n. If True, this layer weights will be restored when\nloading a model\n   name: A name for this layer (optional). Default: 'Dense'.\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nScope\n. This layer scope.\n\n\nW\n: \nTensor\n. Variable representing units weights.\n\n\nb\n: \nTensor\n. Variable representing biases.\n\n\n\n\n\n\nDropout\n\n\ntflearn.layers.core.dropout\n  (incoming,  keep_prob,  name='Dropout')\n\n\nOutputs the input element scaled up by \n1 / keep_prob\n. The scaling is so\nthat the expected sum is unchanged.\n\n\nArguments\n\n\n\n\n\nincoming : A \nTensor\n. The incoming tensor.\n\n\nkeep_prob : A float representing the probability that each element\nis kept.\n\n\nname : A name for this layer (optional).\n\n\n\n\nReferences\n\n\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting.\nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever \n R. Salakhutdinov,\n(2014), Journal of Machine Learning Research, 5(Jun)(2), 1929-1958.\n\n\nLinks\n\n\n\nhttps://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n\n\n\n\nCustom Layer\n\n\ntflearn.layers.core.custom_layer\n  (incoming,  custom_fn,  **kwargs)\n\n\nA custom layer that can apply any operations to the incoming Tensor or\nlist of \nTensor\n. The custom function can be pass as a parameter along\nwith its parameters.\n\n\nArguments\n\n\n\n\n\nincoming : A \nTensor\n or list of \nTensor\n. Incoming tensor.\n\n\ncustom_fn : A custom \nfunction\n, to apply some ops on incoming tensor.\n\n\n**kwargs: Some custom parameters that custom function might need.\n\n\n\n\n\n\nReshape\n\n\ntflearn.layers.core.reshape\n  (incoming,  new_shape,  name='Reshape')\n\n\nA layer that reshape the incoming layer tensor output to the desired shape.\n\n\nArguments\n\n\n\n\n\nincoming\n: A \nTensor\n. The incoming tensor.\n\n\nnew_shape\n: A list of \nint\n. The desired shape.\n\n\nname\n: A name for this layer (optional).\n\n\n\n\n\n\nFlatten\n\n\ntflearn.layers.core.flatten\n  (incoming,  name='Flatten')\n\n\nA shortcut for Reshape layer into 2-D [Batch, Dims].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. The incoming tensor.\n\n\n\n\n\n\nActivation\n\n\ntflearn.layers.core.activation\n  (incoming,  activation='linear')\n\n\nApply given activation to incoming tensor.\n\n\nArguments\n\n\n\n\n\nincoming\n: A \nTensor\n. The incoming tensor.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\n\n\n\n\nSingle Unit\n\n\ntflearn.layers.core.single_unit\n  (incoming,  activation='linear',  bias=True,  trainable=True,  restore=True,  name='Linear')\n\n\nA single unit (Linear) Layer.\n\n\nInput\n\n\n\n1-D Tensor [samples]. If not 2D, input will be flatten.\n\n\nOutput\n\n\n\n1-D Tensor [samples].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming (2+)D Tensor.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model.\n\n\nname\n: A name for this layer (optional). Default: 'Dense'.\n\n\n\n\nAttributes\n\n\n\n\n\nW\n: \nTensor\n. Variable representing weight.\n\n\nb\n: \nTensor\n. Variable representing bias.", 
            "title": "Core Layers"
        }, 
        {
            "location": "/layers/core/#input-data", 
            "text": "tflearn.layers.core.input_data   (shape=None,  placeholder=None,  dtype=tf.float32,  name='InputData')  input_data  is used as a data entry (placeholder) of a network.\nThis placeholder will be feeded with data when training  This layer is used to keep track of the network inputs, by adding the\nplaceholder to INPUTS graphkey. TFLearn training functions may retrieve\nthose variables to setup the network training process.", 
            "title": "Input Data"
        }, 
        {
            "location": "/layers/core/#fully-connected", 
            "text": "tflearn.layers.core.fully_connected   (incoming,  n_units,  activation='linear',  bias=True,  weights_init='truncated_normal',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='Dense')  A fully connected layer.", 
            "title": "Fully Connected"
        }, 
        {
            "location": "/layers/core/#dropout", 
            "text": "tflearn.layers.core.dropout   (incoming,  keep_prob,  name='Dropout')  Outputs the input element scaled up by  1 / keep_prob . The scaling is so\nthat the expected sum is unchanged.", 
            "title": "Dropout"
        }, 
        {
            "location": "/layers/core/#custom-layer", 
            "text": "tflearn.layers.core.custom_layer   (incoming,  custom_fn,  **kwargs)  A custom layer that can apply any operations to the incoming Tensor or\nlist of  Tensor . The custom function can be pass as a parameter along\nwith its parameters.", 
            "title": "Custom Layer"
        }, 
        {
            "location": "/layers/core/#reshape", 
            "text": "tflearn.layers.core.reshape   (incoming,  new_shape,  name='Reshape')  A layer that reshape the incoming layer tensor output to the desired shape.", 
            "title": "Reshape"
        }, 
        {
            "location": "/layers/core/#flatten", 
            "text": "tflearn.layers.core.flatten   (incoming,  name='Flatten')  A shortcut for Reshape layer into 2-D [Batch, Dims].", 
            "title": "Flatten"
        }, 
        {
            "location": "/layers/core/#activation", 
            "text": "tflearn.layers.core.activation   (incoming,  activation='linear')  Apply given activation to incoming tensor.", 
            "title": "Activation"
        }, 
        {
            "location": "/layers/core/#single-unit", 
            "text": "tflearn.layers.core.single_unit   (incoming,  activation='linear',  bias=True,  trainable=True,  restore=True,  name='Linear')  A single unit (Linear) Layer.", 
            "title": "Single Unit"
        }, 
        {
            "location": "/layers/conv/", 
            "text": "Convolution 2D\n\n\ntflearn.layers.conv.conv_2d\n  (incoming,  nb_filter,  filter_size,  strides=1,  padding='same',  activation='linear',  bias=True,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='Conv2D')\n\n\nInput\n\n\n\n4-D Tensor [batch, height, width, in_channels].\n\n\nOutput\n\n\n\n4-D Tensor [batch, new height, new width, nb_filter].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 4-D Tensor.\n\n\nnb_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: 'int\nor list of\nints`. Size of filters.\n\n\nstrides\n: 'int\nor list of\nints`. Strides of conv operation.\nDefault: [1 1 1 1].\n\n\npadding\n: \nstr\n from \n\"same\", \"valid\"\n. Padding algo to use.\nDefault: 'same'.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(see tflearn.initializations) Default: 'truncated_normal'.\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n(see tflearn.initializations) Default: 'zeros'.\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this\nlayer weights (see tflearn.regularizers). Default: None.\n\n\nweight_decay\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model\n\n\nname\n: A name for this layer (optional). Default: 'Conv2D'.\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nScope\n. This layer scope.\n\n\nW\n: \nVariable\n. Variable representing filter weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\n\n\n\n\nConvolution 2D Transpose\n\n\ntflearn.layers.conv.conv_2d_transpose\n  (incoming,  nb_filter,  filter_size,  strides=1,  padding='same',  activation='linear',  bias=True,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='Conv2DTranspose')\n\n\nThis operation is sometimes called \"deconvolution\" after (Deconvolutional\nNetworks)[http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf], but is\nactually the transpose (gradient) of \nconv2d\n rather than an actual\ndeconvolution.\n\n\nInput\n\n\n\n4-D Tensor [batch, height, width, in_channels].\n\n\nOutput\n\n\n\n4-D Tensor [batch, new height, new width, nb_filter].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 4-D Tensor.\n\n\nnb_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: 'int\nor list of\nints`. Size of filters.\n\n\nstrides\n: 'int\nor list of\nints`. Strides of conv operation.\nDefault: [1 1 1 1].\n\n\npadding\n: \nstr\n from \n\"same\", \"valid\"\n. Padding algo to use.\nDefault: 'same'.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(see tflearn.initializations) Default: 'truncated_normal'.\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n(see tflearn.initializations) Default: 'zeros'.\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this\nlayer weights (see tflearn.regularizers). Default: None.\n\n\nweight_decay\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model\n\n\nname\n: A name for this layer (optional). Default: 'Conv2DTranspose'.\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nScope\n. This layer scope.\n\n\nW\n: \nVariable\n. Variable representing filter weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\n\n\n\n\nMax Pooling 2D\n\n\ntflearn.layers.conv.max_pool_2d\n  (incoming,  kernel_size,  strides=None,  padding='same',  name='MaxPool2D')\n\n\nInput\n\n\n\n4-D Tensor [batch, height, width, in_channels].\n\n\nOutput\n\n\n\n4-D Tensor [batch, pooled height, pooled width, in_channels].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 4-D Layer.\n\n\nkernel_size\n: 'int\nor list of\nints`. Pooling kernel size.\n\n\nstrides\n: 'int\nor list of\nints`. Strides of conv operation.\nDefault: same as kernel_size.\n\n\npadding\n: \nstr\n from \n\"same\", \"valid\"\n. Padding algo to use.\nDefault: 'same'.\n\n\nname\n: A name for this layer (optional). Default: 'MaxPool2D'.\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nScope\n. This layer scope.\n\n\n\n\n\n\nAverage Pooling 2D\n\n\ntflearn.layers.conv.avg_pool_2d\n  (incoming,  kernel_size,  strides=None,  padding='same',  name='AvgPool2D')\n\n\nInput\n\n\n\n4-D Tensor [batch, height, width, in_channels].\n\n\nOutput\n\n\n\n4-D Tensor [batch, pooled height, pooled width, in_channels].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 4-D Layer.\n\n\nkernel_size\n: 'int\nor list of\nints`. Pooling kernel size.\n\n\nstrides\n: 'int\nor list of\nints`. Strides of conv operation.\nDefault: same as kernel_size.\n\n\npadding\n: \nstr\n from \n\"same\", \"valid\"\n. Padding algo to use.\nDefault: 'same'.\n\n\nname\n: A name for this layer (optional). Default: 'AvgPool2D'.\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nScope\n. This layer scope.\n\n\n\n\n\n\nConvolution 1D\n\n\ntflearn.layers.conv.conv_1d\n  (incoming,  nb_filter,  filter_size,  strides=1,  padding='same',  activation='linear',  bias=True,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='Conv1D')\n\n\nInput\n\n\n\n3-D Tensor [batch, steps, in_channels].\n\n\nOutput\n\n\n\n3-D Tensor [batch, new steps, nb_filters].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 3-D Tensor.\n\n\nnb_filter\n: \nint\n. The number of convolutional filters.\n\n\nfilter_size\n: 'int\nor list of\nints`. Size of filters.\n\n\nstrides\n: 'int\nor list of\nints`. Strides of conv operation.\nDefault: [1 1 1 1].\n\n\npadding\n: \nstr\n from \n\"same\", \"valid\"\n. Padding algo to use.\nDefault: 'same'.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(see tflearn.initializations) Default: 'truncated_normal'.\n\n\nbias_init\n: \nstr\n (name) or \nTensor\n. Bias initialization.\n(see tflearn.initializations) Default: 'zeros'.\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this\nlayer weights (see tflearn.regularizers). Default: None.\n\n\nweight_decay\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model\n\n\nname\n: A name for this layer (optional). Default: 'Conv1D'.\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nScope\n. This layer scope.\n\n\nW\n: \nVariable\n. Variable representing filter weights.\n\n\nb\n: \nVariable\n. Variable representing biases.\n\n\n\n\n\n\nMax Pooling 1D\n\n\ntflearn.layers.conv.max_pool_1d\n  (incoming,  kernel_size,  strides=None,  padding='same',  name='MaxPool2D')\n\n\nInput\n\n\n\n3-D Tensor [batch, steps, in_channels].\n\n\nOutput\n\n\n\n3-D Tensor [batch, pooled steps, in_channels].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 3-D Layer.\n\n\nkernel_size\n: 'int\nor list of\nints`. Pooling kernel size.\n\n\nstrides\n: 'int\nor list of\nints`. Strides of conv operation.\nDefault: same as kernel_size.\n\n\npadding\n: \nstr\n from \n\"same\", \"valid\"\n. Padding algo to use.\nDefault: 'same'.\n\n\nname\n: A name for this layer (optional). Default: 'MaxPool1D'.\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nScope\n. This layer scope.\n\n\n\n\n\n\nAverage Pooling 1D\n\n\ntflearn.layers.conv.avg_pool_1d\n  (incoming,  kernel_size,  strides=None,  padding='same',  name='AvgPool2D')\n\n\nInput\n\n\n\n3-D Tensor [batch, steps, in_channels].\n\n\nOutput\n\n\n\n3-D Tensor [batch, pooled steps, in_channels].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 3-D Layer.\n\n\nkernel_size\n: 'int\nor list of\nints`. Pooling kernel size.\n\n\nstrides\n: 'int\nor list of\nints`. Strides of conv operation.\nDefault: same as kernel_size.\n\n\npadding\n: \nstr\n from \n\"same\", \"valid\"\n. Padding algo to use.\nDefault: 'same'.\n\n\nname\n: A name for this layer (optional). Default: 'AvgPool1D'.\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nScope\n. This layer scope.\n\n\n\n\n\n\nDeep Bottleneck\n\n\ntflearn.layers.conv.deep_bottleneck\n  (incoming,  nb_layers,  nb_filter,  bottlenet_size,  activation='relu',  batch_norm=True,  bias=False,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='DeepBottleneck')\n\n\nAs described in MSRA's Deep Residual Network paper.\n\n\nInput\n\n\n\n4-D Tensor [batch, height, width, in_channels].\n\n\nOutput\n\n\n\n4-D Tensor [batch, new height, new width, nb_filter].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 4-D Layer.\n\n\nnb_layers\n: \nint\n. Number of layer blocks.\n\n\nnb_filter\n: \nint\n. The number of convolutional filters of the\nlayers surrounding the bottleneck layer.\n\n\nbottlenet_size\n: \nint\n. The number of convolutional filter of the\nbottleneck convolutional layer.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\nbatch_norm\n: \nbool\n. If True, apply batch normalization.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(see tflearn.initializations) Default: 'uniform_scaling'.\n\n\nbias_init\n: \nstr\n (name) or \ntf.Tensor\n. Bias initialization.\n(see tflearn.initializations) Default: 'zeros'.\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this\nlayer weights (see tflearn.regularizers). Default: None.\n\n\nweight_decay\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model\n\n\nname\n: A name for this layer (optional). Default: 'DeepBottleneck'.\n\n\n\n\n\n\nDeep Residual Block\n\n\ntflearn.layers.conv.deep_residual_block\n  (incoming,  nb_blocks,  out_channels,  downsample=False,  downsample_strides=2,  activation='relu',  batch_norm=True,  bias=False,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='DeepResidualBlock')\n\n\nA deep residual block as described in MSRA's Deep Residual Network paper.\n\n\nNotice: Because TensorFlow doesn't support a strides \n filter size,\nan average pooling is used as a fix, but decrease performances.\n\n\nInput\n\n\n\n4-D Tensor [batch, height, width, in_channels].\n\n\nOutput\n\n\n\n4-D Tensor [batch, new height, new width, nb_filter].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 4-D Layer.\n\n\nnb_blocks\n: \nint\n. Number of layer blocks.\n\n\nout_channels\n: \nint\n. The number of convolutional filters of the\nlayers surrounding the bottleneck layer.\n\n\ndownsample\n:\n\n\ndownsample_strides\n:\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\nbatch_norm\n: \nbool\n. If True, apply batch normalization.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(see tflearn.initializations) Default: 'uniform_scaling'.\n\n\nbias_init\n: \nstr\n (name) or \ntf.Tensor\n. Bias initialization.\n(see tflearn.initializations) Default: 'zeros'.\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this\nlayer weights (see tflearn.regularizers). Default: None.\n\n\nweight_decay\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model\n\n\nname\n: A name for this layer (optional). Default: 'DeepBottleneck'.\n\n\n\n\nReferences\n\n\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu\nZhang, Shaoqing Ren, Jian Sun. 2015.\n\n\nLinks\n\n\n\nhttp://arxiv.org/pdf/1512.03385v1.pdf\n\n\n\n\nShallow Residual Block\n\n\ntflearn.layers.conv.shallow_residual_block\n  (incoming,  nb_blocks,  out_channels,  downsample=False,  downsample_strides=2,  activation='relu',  batch_norm=True,  bias=False,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.0001,  trainable=True,  restore=True,  name='ShallowResidualBlock')\n\n\nA shallow residual block as described in MSRA's Deep Residual Network\npaper.\n\n\nNotice: Because TensorFlow doesn't support a strides \n filter size,\nan average pooling is used as a fix, but decrease performances.\n\n\nInput\n\n\n\n4-D Tensor [batch, height, width, in_channels].\n\n\nOutput\n\n\n\n4-D Tensor [batch, new height, new width, nb_filter].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 4-D Layer.\n\n\nnb_blocks\n: \nint\n. Number of layer blocks.\n\n\nout_channels\n: \nint\n. The number of convolutional filters of the\nconvolution layers.\n\n\ndownsample\n: \nbool\n. If True, apply downsampling using\n'downsample_strides' for strides.\n\n\ndownsample_strides\n: \nint\n. The strides to use when downsampling.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(see tflearn.activations). Default: 'linear'.\n\n\nbatch_norm\n: \nbool\n. If True, apply batch normalization.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(see tflearn.initializations) Default: 'uniform_scaling'.\n\n\nbias_init\n: \nstr\n (name) or \ntf.Tensor\n. Bias initialization.\n(see tflearn.initializations) Default: 'zeros'.\n\n\nregularizer\n: \nstr\n (name) or \nTensor\n. Add a regularizer to this\nlayer weights (see tflearn.regularizers). Default: None.\n\n\nweight_decay\n: \nfloat\n. Regularizer decay parameter. Default: 0.001.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model\n\n\nname\n: A name for this layer (optional). Default: 'ShallowBottleneck'.\n\n\n\n\nReferences\n\n\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu\nZhang, Shaoqing Ren, Jian Sun. 2015.\n\n\nLinks\n\n\n\nhttp://arxiv.org/pdf/1512.03385v1.pdf", 
            "title": "Convolutional Layers"
        }, 
        {
            "location": "/layers/conv/#convolution-2d", 
            "text": "tflearn.layers.conv.conv_2d   (incoming,  nb_filter,  filter_size,  strides=1,  padding='same',  activation='linear',  bias=True,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='Conv2D')", 
            "title": "Convolution 2D"
        }, 
        {
            "location": "/layers/conv/#convolution-2d-transpose", 
            "text": "tflearn.layers.conv.conv_2d_transpose   (incoming,  nb_filter,  filter_size,  strides=1,  padding='same',  activation='linear',  bias=True,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='Conv2DTranspose')  This operation is sometimes called \"deconvolution\" after (Deconvolutional\nNetworks)[http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf], but is\nactually the transpose (gradient) of  conv2d  rather than an actual\ndeconvolution.", 
            "title": "Convolution 2D Transpose"
        }, 
        {
            "location": "/layers/conv/#max-pooling-2d", 
            "text": "tflearn.layers.conv.max_pool_2d   (incoming,  kernel_size,  strides=None,  padding='same',  name='MaxPool2D')", 
            "title": "Max Pooling 2D"
        }, 
        {
            "location": "/layers/conv/#average-pooling-2d", 
            "text": "tflearn.layers.conv.avg_pool_2d   (incoming,  kernel_size,  strides=None,  padding='same',  name='AvgPool2D')", 
            "title": "Average Pooling 2D"
        }, 
        {
            "location": "/layers/conv/#convolution-1d", 
            "text": "tflearn.layers.conv.conv_1d   (incoming,  nb_filter,  filter_size,  strides=1,  padding='same',  activation='linear',  bias=True,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='Conv1D')", 
            "title": "Convolution 1D"
        }, 
        {
            "location": "/layers/conv/#max-pooling-1d", 
            "text": "tflearn.layers.conv.max_pool_1d   (incoming,  kernel_size,  strides=None,  padding='same',  name='MaxPool2D')", 
            "title": "Max Pooling 1D"
        }, 
        {
            "location": "/layers/conv/#average-pooling-1d", 
            "text": "tflearn.layers.conv.avg_pool_1d   (incoming,  kernel_size,  strides=None,  padding='same',  name='AvgPool2D')", 
            "title": "Average Pooling 1D"
        }, 
        {
            "location": "/layers/conv/#deep-bottleneck", 
            "text": "tflearn.layers.conv.deep_bottleneck   (incoming,  nb_layers,  nb_filter,  bottlenet_size,  activation='relu',  batch_norm=True,  bias=False,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='DeepBottleneck')  As described in MSRA's Deep Residual Network paper.", 
            "title": "Deep Bottleneck"
        }, 
        {
            "location": "/layers/conv/#deep-residual-block", 
            "text": "tflearn.layers.conv.deep_residual_block   (incoming,  nb_blocks,  out_channels,  downsample=False,  downsample_strides=2,  activation='relu',  batch_norm=True,  bias=False,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  name='DeepResidualBlock')  A deep residual block as described in MSRA's Deep Residual Network paper.  Notice: Because TensorFlow doesn't support a strides   filter size,\nan average pooling is used as a fix, but decrease performances.", 
            "title": "Deep Residual Block"
        }, 
        {
            "location": "/layers/conv/#shallow-residual-block", 
            "text": "tflearn.layers.conv.shallow_residual_block   (incoming,  nb_blocks,  out_channels,  downsample=False,  downsample_strides=2,  activation='relu',  batch_norm=True,  bias=False,  weights_init='uniform_scaling',  bias_init='zeros',  regularizer=None,  weight_decay=0.0001,  trainable=True,  restore=True,  name='ShallowResidualBlock')  A shallow residual block as described in MSRA's Deep Residual Network\npaper.  Notice: Because TensorFlow doesn't support a strides   filter size,\nan average pooling is used as a fix, but decrease performances.", 
            "title": "Shallow Residual Block"
        }, 
        {
            "location": "/layers/recurrent/", 
            "text": "Simple RNN\n\n\ntflearn.layers.recurrent.simple_rnn\n  (incoming,  n_units,  activation='sigmoid',  bias=True,  weights_init='truncated_normal',  return_seq=False,  trainable=True,  restore=True,  name='SimpleRNN')\n\n\nSimple Recurrent Layer.\n\n\nInput\n\n\n\n3-D Tensor [samples, timesteps, input dim].\n\n\nOutput\n\n\n\nif \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\nelse: 2-D Tensor [samples, output dim].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 3-D Tensor.\n\n\nn_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(See tflearn.activations). Default: 'sigmoid'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(See tflearn.initializations) Default: 'truncated_normal'.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\nlast sequence output only.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\nLSTM\n\n\ntflearn.layers.recurrent.lstm\n  (incoming,  n_units,  activation='sigmoid',  inner_activation='tanh',  bias=True,  weights_init='truncated_normal',  forget_bias=1.0,  return_seq=False,  trainable=True,  restore=True,  name='LSTM')\n\n\nLong Short Term Memory Recurrent Layer.\n\n\nInput\n\n\n\n3-D Tensor [samples, timesteps, input dim].\n\n\nOutput\n\n\n\nif \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\nelse: 2-D Tensor [samples, output dim].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 3-D Tensor.\n\n\nn_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name) or \nTensor\n. Activation applied to this layer.\n(See tflearn.activations). Default: 'sigmoid'.\n\n\ninner_activation\n: \nstr\n (name) or \nTensor\n. LSTM inner activation.\nDefault: 'tanh'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(See tflearn.initializations) Default: 'truncated_normal'.\n\n\nforget_bias\n: \nfloat\n. Bias of the forget gate. Default: 1.0.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\nlast sequence output only.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\nReferences\n\n\n\nLong Short Term Memory, Sepp Hochreiter \n Jurgen Schmidhuber,\nNeural Computation 9(8): 1735-1780, 1997.\n\n\nLinks\n\n\n\nhttp://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n\n\n\n\nGRU\n\n\ntflearn.layers.recurrent.gru\n  (incoming,  n_units,  activation='sigmoid',  inner_activation='tanh',  bias=True,  weights_init='truncated_normal',  return_seq=False,  trainable=True,  restore=True,  name='GRU')\n\n\nGated Recurrent Unit Layer.\n\n\nInput\n\n\n\n3-D Tensor Layer [samples, timesteps, input dim].\n\n\nOutput\n\n\n\nif \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\nelse: 2-D Tensor [samples, output dim].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 3-D Tensor.\n\n\nn_units\n: \nint\n, number of units for this layer.\n\n\nactivation\n: \nstr\n (name). Activation applied to this layer.\n(See tflearn.activations). Default: 'sigmoid'.\n\n\ninner_activation\n: \nstr\n (name) or \nTensor\n. GRU inner activation.\nDefault: 'tanh'.\n\n\nbias\n: \nbool\n. If True, a bias is used.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(See tflearn.initializations) Default: 'truncated_normal'.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\nlast sequence output only.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\nReferences\n\n\n\nLearning Phrase Representations using RNN Encoder\u2013Decoder for\nStatistical Machine Translation, K. Cho et al., 2014.\n\n\nLinks\n\n\n\nhttp://arxiv.org/abs/1406.1078\n\n\n\n\nBidirectional RNN\n\n\ntflearn.layers.recurrent.bidirectional_rnn\n  (incoming,  rnncell_fw,  rnncell_bw,  return_seq=False,  name='BidirectionalRNN')\n\n\nBuild a bidirectional recurrent neural network, it requires 2 RNN Cells\nto process sequence in forward and backward order. Any RNN Cell can be\nused i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two\ncells number of units must match.\n\n\nInput\n\n\n\n3-D Tensor Layer [samples, timesteps, input dim].\n\n\nOutput\n\n\n\nif \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\nelse: 2-D Tensor Layer [samples, output dim].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. The incoming Tensor.\n\n\nrnncell_fw\n: \nRNNCell\n. The RNN Cell to use for foward computation.\n\n\nrnncell_bw\n: \nRNNCell\n. The RNN Cell to use for backward computation.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\nlast sequence output only.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\n\n\nDynamic RNN\n\n\ntflearn.layers.recurrent.dynamic_rnn\n  (incoming,  rnncell,  sequence_length=None,  time_major=False,  return_seq=False,  name='DynamicRNN')\n\n\nRNN with dynamic sequence length.\n\n\nUnlike \nrnn\n, the input \ninputs\n is not a Python list of \nTensors\n.\nInstead, it is a single \nTensor\n where the maximum time is either the\nfirst or second dimension (see the parameter \ntime_major\n).  The\ncorresponding output is a single \nTensor\n having the same number of time\nsteps and batch size.\n\n\nThe parameter \nsequence_length\n is required and dynamic calculation is\nautomatically performed.\n\n\nInput\n\n\n\n3-D Tensor Layer [samples, timesteps, input dim].\n\n\nOutput\n\n\n\nif \nreturn_seq\n: 3-D Tensor [samples, timesteps, output dim].\nelse: 2-D Tensor Layer [samples, output dim].\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. The incoming 3-D Tensor.\n\n\nrnncell\n: \nRNNCell\n. The RNN Cell to use for computation.\n\n\nreturn_seq\n: \nbool\n. If True, returns the full sequence instead of\nlast sequence output only.\n\n\nname\n: \nstr\n. A name for this layer (optional).", 
            "title": "Recurrent Layers"
        }, 
        {
            "location": "/layers/recurrent/#simple-rnn", 
            "text": "tflearn.layers.recurrent.simple_rnn   (incoming,  n_units,  activation='sigmoid',  bias=True,  weights_init='truncated_normal',  return_seq=False,  trainable=True,  restore=True,  name='SimpleRNN')  Simple Recurrent Layer.", 
            "title": "Simple RNN"
        }, 
        {
            "location": "/layers/recurrent/#lstm", 
            "text": "tflearn.layers.recurrent.lstm   (incoming,  n_units,  activation='sigmoid',  inner_activation='tanh',  bias=True,  weights_init='truncated_normal',  forget_bias=1.0,  return_seq=False,  trainable=True,  restore=True,  name='LSTM')  Long Short Term Memory Recurrent Layer.", 
            "title": "LSTM"
        }, 
        {
            "location": "/layers/recurrent/#gru", 
            "text": "tflearn.layers.recurrent.gru   (incoming,  n_units,  activation='sigmoid',  inner_activation='tanh',  bias=True,  weights_init='truncated_normal',  return_seq=False,  trainable=True,  restore=True,  name='GRU')  Gated Recurrent Unit Layer.", 
            "title": "GRU"
        }, 
        {
            "location": "/layers/recurrent/#bidirectional-rnn", 
            "text": "tflearn.layers.recurrent.bidirectional_rnn   (incoming,  rnncell_fw,  rnncell_bw,  return_seq=False,  name='BidirectionalRNN')  Build a bidirectional recurrent neural network, it requires 2 RNN Cells\nto process sequence in forward and backward order. Any RNN Cell can be\nused i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two\ncells number of units must match.", 
            "title": "Bidirectional RNN"
        }, 
        {
            "location": "/layers/recurrent/#dynamic-rnn", 
            "text": "tflearn.layers.recurrent.dynamic_rnn   (incoming,  rnncell,  sequence_length=None,  time_major=False,  return_seq=False,  name='DynamicRNN')  RNN with dynamic sequence length.  Unlike  rnn , the input  inputs  is not a Python list of  Tensors .\nInstead, it is a single  Tensor  where the maximum time is either the\nfirst or second dimension (see the parameter  time_major ).  The\ncorresponding output is a single  Tensor  having the same number of time\nsteps and batch size.  The parameter  sequence_length  is required and dynamic calculation is\nautomatically performed.", 
            "title": "Dynamic RNN"
        }, 
        {
            "location": "/layers/normalization/", 
            "text": "Batch Normalization\n\n\ntflearn.layers.normalization.batch_normalization\n  (incoming,  beta=0.0,  gamma=1.0,  epsilon=1e-05,  decay=0.999,  trainable=True,  restore=True,  stddev=0.002,  name='BatchNormalization')\n\n\nNormalize activations of the previous layer at each batch.\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming Tensor.\n\n\nbeta\n: \nfloat\n. Default: 0.0.\n\n\ngamma\n: \nfloat\n. Default: 1.0.\n\n\nepsilon\n: \nfloat\n. Defalut: 1e-5.\n\n\ndecay\n: \nfloat\n. Default: 0.999.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model.\n\n\nstddev\n: \nfloat\n. Standard deviation for weights initialization.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\nReferences\n\n\n\nBatch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shif. Sergey Ioffe, Christian Szegedy. 2015.\n\n\nLinks\n\n\n\nhttp://arxiv.org/pdf/1502.03167v3.pdf\n\n\n\n\nLocal Response Normalization\n\n\ntflearn.layers.normalization.local_response_normalization\n  (incoming,  depth_radius=5,  bias=1.0,  alpha=0.0001,  beta=0.75,  name='LocalResponseNormalization')\n\n\nInput\n\n\n\n4-D Tensor Layer.\n\n\nOutput\n\n\n\n4-D Tensor Layer. (Same dimension as input).\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming Tensor.\n\n\ndepth_radius\n: \nint\n. 0-D.  Half-width of the 1-D normalization window.\nDefaults to 5.\n\n\nbias\n: \nfloat\n. An offset (usually positive to avoid dividing by 0).\nDefaults to 1.0.\n\n\nalpha\n: \nfloat\n. A scale factor, usually positive. Defaults to 0.0001.\n\n\nbeta\n: \nfloat\n. An exponent. Defaults to \n0.5\n.", 
            "title": "Normalization Layers"
        }, 
        {
            "location": "/layers/normalization/#batch-normalization", 
            "text": "tflearn.layers.normalization.batch_normalization   (incoming,  beta=0.0,  gamma=1.0,  epsilon=1e-05,  decay=0.999,  trainable=True,  restore=True,  stddev=0.002,  name='BatchNormalization')  Normalize activations of the previous layer at each batch.", 
            "title": "Batch Normalization"
        }, 
        {
            "location": "/layers/normalization/#local-response-normalization", 
            "text": "tflearn.layers.normalization.local_response_normalization   (incoming,  depth_radius=5,  bias=1.0,  alpha=0.0001,  beta=0.75,  name='LocalResponseNormalization')", 
            "title": "Local Response Normalization"
        }, 
        {
            "location": "/layers/embedding_ops/", 
            "text": "Embedding\n\n\ntflearn.layers.embedding_ops.embedding\n  (incoming,  input_dim,  output_dim,  weights_init='truncated_normal',  trainable=True,  restore=True,  name='Embedding')\n\n\nEmbedding layer for a sequence of ids.\n\n\nInput\n\n\n\n2-D Tensor (List of ids).\n\n\nOutput\n\n\n\n2-D Tensor.\n\n\nArguments\n\n\n\n\n\nincoming\n: Incoming 2D Tensor.\n\n\ninput_dim\n: list of \nint\n. Vocabulary size (number of ids).\n\n\noutput_dim\n: list of \nint\n. Embedding size.\n\n\nweights_init\n: \nstr\n (name) or \nTensor\n. Weights initialization.\n(see tflearn.initializations) Default: 'truncated_normal'.\n\n\ntrainable\n: \nbool\n. If True, weights will be trainable.\n\n\nrestore\n: \nbool\n. If True, this layer weights will be restored when\nloading a model\n\n\nname\n: A name for this layer (optional). Default: 'Embedding'.", 
            "title": "Embedding Layers"
        }, 
        {
            "location": "/layers/embedding_ops/#embedding", 
            "text": "tflearn.layers.embedding_ops.embedding   (incoming,  input_dim,  output_dim,  weights_init='truncated_normal',  trainable=True,  restore=True,  name='Embedding')  Embedding layer for a sequence of ids.", 
            "title": "Embedding"
        }, 
        {
            "location": "/layers/merge_ops/", 
            "text": "Merge\n\n\ntflearn.layers.merge_ops.merge\n  (tensors_list,  mode,  axis=1,  name='Merge')\n\n\nMerge a list of \nTensor\n into a single one.\n\n\nInput\n\n\n\nList of Tensors.\n\n\nOutput\n\n\n\nMerged Tensors.\n\n\nArguments\n\n\n\n\n\ntensors_list\n: A list of \nTensor\n, A list of tensors to merge.\n\n\nmode\n: \nstr\n. Merging mode, it supports:\n'concat': concatenate outputs along specified axis\n'elemwise_sum': outputs element-wise sum.\n'elemwise_mul': outputs element-wise sum.\n'sum': outputs element-wise sum along specified axis\n'mean': outputs element-wise average along specified axis\n'prod': outputs element-wise multiplication along specified axis\n'max': outputs max elements along specified axis\n'min': outputs min elements along specified axis\n'and': \nlogical and\n btw outputs elements along specified axis\n'or': \nlogical or\n btw outputs elements along specified axis\n\n\naxis\n: \nint\n. Represents the axis to use for merging mode.\nIn most cases: 0 for concat and 1 for other modes.\n\n\nname\n: A name for this layer (optional). Default: 'Merge'.\n\n\n\n\n\n\nMerge Outputs\n\n\ntflearn.layers.merge_ops.merge_outputs\n  (tensor_list,  name='MergeOutputs')\n\n\nA layer that concatenate all outputs of a network into a single tensor.\n\n\nInput\n\n\n\nList of Tensors [\nshape\n].\n\n\nOutput\n\n\n\nConcatenated Tensors [nb_tensors, \nshape\n].\n\n\nArguments\n\n\n\n\n\ntensor_list\n: list of \nTensor\n. The network outputs.\n\n\nname\n: \nstr\n. A name for this layer (optional).\n\n\n\n\nReturns\n\n\n\nA \nTensor\n.", 
            "title": "Merge Layers"
        }, 
        {
            "location": "/layers/merge_ops/#merge", 
            "text": "tflearn.layers.merge_ops.merge   (tensors_list,  mode,  axis=1,  name='Merge')  Merge a list of  Tensor  into a single one.", 
            "title": "Merge"
        }, 
        {
            "location": "/layers/merge_ops/#merge-outputs", 
            "text": "tflearn.layers.merge_ops.merge_outputs   (tensor_list,  name='MergeOutputs')  A layer that concatenate all outputs of a network into a single tensor.", 
            "title": "Merge Outputs"
        }, 
        {
            "location": "/layers/estimator/", 
            "text": "Regression\n\n\ntflearn.layers.estimator.regression\n  (incoming,  placeholder=None,  optimizer='adam',  loss='categorical_crossentropy',  metric='default',  learning_rate=0.001,  dtype=tf.float32,  batch_size=64,  shuffle_batches=True,  op_name=None,  name=None)\n\n\nInput\n\n\n\n2-D Tensor Layer.\n\n\nOutput\n\n\n\n2-D Tensor Layer (Same as input).\n\n\nArguments\n\n\n\n\n\nincoming\n: \nTensor\n. Incoming 2-D Tensor.\n\n\nplaceholder\n: \nTensor\n. This regression target (label) placeholder.\nIf 'None' provided, a placeholder will be added automatically.\nYou can retrieve that placeholder through graph key: 'TARGETS',\nor the 'placeholder' attribute of this function's returned tensor.\n\n\noptimizer\n: \nstr\n (name) or \nOptimizer\n. Optimizer to use.\nDefault: 'sgd' (Stochastic Descent Gradient).\n\n\nloss\n: \nstr\n (name) or \nTensor\n. Loss function used by this layer\noptimizer. Default: 'categorical_crossentropy'.\n\n\nmetric\n: \nstr\n, \nMetric\n or \nTensor\n. The metric to be used.\nDefault: 'default' metric is 'accuracy'. To disable metric\ncalculation, set it to 'None'.\n\n\nlearning_rate\n: \nfloat\n. This layer optimizer's learning rate.\n\n\ndtype\n: \ntf.types\n. This layer placeholder type. Default: tf.float32.\n\n\nbatch_size\n: \nint\n. Batch size of data to use for training. tflearn\nsupports different batch size for every optimizers. Default: 64.\n\n\nshuffle_batches\n: \nbool\n. Shuffle or not this optimizer batches at\nevery epoch. Default: True.\n\n\nop_name\n: A name for this layer optimizer (optional).\nDefault: optimizer op name.\n\n\nname\n: A name for this layer's placeholder scope.\n\n\n\n\nAttributes\n\n\n\n\n\nplaceholder\n: \nTensor\n. Placeholder for feeding labels.", 
            "title": "Estimator Layers"
        }, 
        {
            "location": "/layers/estimator/#regression", 
            "text": "tflearn.layers.estimator.regression   (incoming,  placeholder=None,  optimizer='adam',  loss='categorical_crossentropy',  metric='default',  learning_rate=0.001,  dtype=tf.float32,  batch_size=64,  shuffle_batches=True,  op_name=None,  name=None)", 
            "title": "Regression"
        }, 
        {
            "location": "/activations/", 
            "text": "Linear\n\n\ntflearn.activations.linear\n  (x)\n\n\nf(x) = x\n\n\nArguments\n\n\n\n\n\nx : A \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n,\n\nint16\n, or \nint8\n.\n\n\n\n\nReturns\n\n\n\nThe incoming Tensor (without changes).\n\n\n\n\nTanh\n\n\ntflearn.activations.tanh\n  (x)\n\n\nComputes hyperbolic tangent of \nx\n element-wise.\n\n\nArguments\n\n\n\n\n\nx\n: A Tensor with type \nfloat\n, \ndouble\n, \nint32\n, \ncomplex64\n, \nint64\n,\nor \nqint32\n.\n\n\n\n\nReturns\n\n\n\nA Tensor with the same type as \nx\n if \nx.dtype != qint32\n otherwise\n  the return type is \nquint8\n.\n\n\n\n\nSigmoid\n\n\ntflearn.activations.sigmoid\n  (x)\n\n\nComputes sigmoid of \nx\n element-wise.\nSpecifically, \ny = 1 / (1 + exp(-x))\n.\n\n\nArguments\n\n\n\n\n\nx\n: A Tensor with type \nfloat\n, \ndouble\n, \nint32\n, \ncomplex64\n, \nint64\n,\nor \nqint32\n.\n\n\n\n\nReturns\n\n\n\nA Tensor with the same type as \nx\n if \nx.dtype != qint32\n otherwise\nthe return type is \nquint8\n.\n\n\n\n\nSoftmax\n\n\ntflearn.activations.softmax\n  (x)\n\n\nComputes softmax activations.\n\n\nFor each batch \ni\n and class \nj\n we have\n\n\nsoftmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))\n\n\nArguments\n\n\n\n\n\nx\n: A \nTensor\n. Must be one of the following types: \nfloat32\n,\n\nfloat64\n. 2-D with shape \n[batch_size, num_classes]\n.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n. Has the same type as \nx\n. Same shape as \nx\n.\n\n\n\n\nSoftplus\n\n\ntflearn.activations.softplus\n  (x)\n\n\nComputes softplus: \nlog(exp(features) + 1)\n.\n\n\nArguments\n\n\n\n\n\nx\n: A \nTensor\n. Must be one of the following types: \nfloat32\n,\n\nfloat64\n, \nint32\n, \nint64\n, \nuint8\n, \nint16\n, \nint8\n, \nuint16\n.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n. Has the same type as \nx\n.\n\n\n\n\nSoftsign\n\n\ntflearn.activations.softsign\n  (x)\n\n\nComputes softsign: \nfeatures / (abs(features) + 1)\n.\n\n\nArguments\n\n\n\n\n\nx\n: A \nTensor\n. Must be one of the following types: \nfloat32\n,\n\nfloat64\n, \nint32\n, \nint64\n, \nuint8\n, \nint16\n, \nint8\n, \nuint16\n.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n. Has the same type as \nx\n.\n\n\n\n\nReLU\n\n\ntflearn.activations.relu\n  (x)\n\n\nComputes rectified linear: \nmax(features, 0)\n.\n\n\nArguments\n\n\n\n\n\nx\n: A \nTensor\n. Must be one of the following types: \nfloat32\n,\n\nfloat64\n, \nint32\n, \nint64\n, \nuint8\n, \nint16\n, \nint8\n, \nuint16\n.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n. Has the same type as \nx\n.\n\n\n\n\nReLU6\n\n\ntflearn.activations.relu6\n  (x)\n\n\nComputes Rectified Linear 6: \nmin(max(features, 0), 6)\n.\n\n\nArguments\n\n\n\n\n\nx\n: A \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n,\n\nint16\n, or \nint8\n.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n with the same type as \nx\n.\n\n\n\n\nLeakyReLU\n\n\ntflearn.activations.leaky_relu\n  (x,  alpha=0.1,  name='LeakyReLU')\n\n\nModified version of ReLU, introducing a nonzero gradient for negative\ninput.\n\n\nArguments\n\n\n\n\n\nx\n: A \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n,\n\nint16\n, or \nint8\n.\n\n\nalpha\n: \nfloat\n. slope.\n\n\nname\n: A name for this activation op (optional).\n\n\n\n\nReturns\n\n\n\nA \nTensor\n with the same type as \nx\n.\n\n\nReferences\n\n\n\nRectifier Nonlinearities Improve Neural Network Acoustic Models,\nMaas et al. (2013).\n\n\nLinks\n\n\n\nhttp://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf\n\n\n\n\nPReLU\n\n\ntflearn.activations.prelu\n  (x,  weights_init='zeros',  name='PReLU')\n\n\nParametric Rectified Linear Unit.\n\n\nArguments\n\n\n\n\n\nx\n: A \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n,\n\nint16\n, or \nint8\n.\n\n\nweights_init\n: \nstr\n. Weights initialization. Default: zeros.\n\n\nname\n: A name for this activation op (optional).\n\n\n\n\nAttributes\n\n\n\n\n\nscope\n: \nstr\n. This op scope.\n\n\nalphas\n: \nVariable\n. PReLU alphas.\n\n\n\n\nReturns\n\n\n\nA \nTensor\n with the same type as \nx\n.\n\n\nReferences\n\n\n\nDelving Deep into Rectifiers: Surpassing Human-Level Performance\non ImageNet Classification. He et al., 2014.\n\n\nLinks\n\n\n\nhttp://arxiv.org/pdf/1502.01852v1.pdf\n\n\n\n\nELU\n\n\ntflearn.activations.elu\n  (x)\n\n\nExponential Linear Unit.\n\n\nArguments\n\n\n\n\n\nx : A \nTensor\n with type \nfloat\n, \ndouble\n, \nint32\n, \nint64\n, \nuint8\n,\n\nint16\n, or \nint8\n.\n\n\nname : A name for this activation op (optional).\n\n\n\n\nReturns\n\n\n\nA \ntuple\n of \ntf.Tensor\n. This layer inference, i.e. output Tensors\nat training and testing time.\n\n\nReferences\n\n\n\nFast and Accurate Deep Network Learning by Exponential Linear Units,\nDjork-Arn\u00e9 Clevert, Thomas Unterthiner, Sepp Hochreiter. 2015.\n\n\nLinks\n\n\n\nhttp://arxiv.org/abs/1511.07289", 
            "title": "Activations"
        }, 
        {
            "location": "/activations/#linear", 
            "text": "tflearn.activations.linear   (x)  f(x) = x", 
            "title": "Linear"
        }, 
        {
            "location": "/activations/#tanh", 
            "text": "tflearn.activations.tanh   (x)  Computes hyperbolic tangent of  x  element-wise.", 
            "title": "Tanh"
        }, 
        {
            "location": "/activations/#sigmoid", 
            "text": "tflearn.activations.sigmoid   (x)  Computes sigmoid of  x  element-wise.\nSpecifically,  y = 1 / (1 + exp(-x)) .", 
            "title": "Sigmoid"
        }, 
        {
            "location": "/activations/#softmax", 
            "text": "tflearn.activations.softmax   (x)  Computes softmax activations.  For each batch  i  and class  j  we have  softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))", 
            "title": "Softmax"
        }, 
        {
            "location": "/activations/#softplus", 
            "text": "tflearn.activations.softplus   (x)  Computes softplus:  log(exp(features) + 1) .", 
            "title": "Softplus"
        }, 
        {
            "location": "/activations/#softsign", 
            "text": "tflearn.activations.softsign   (x)  Computes softsign:  features / (abs(features) + 1) .", 
            "title": "Softsign"
        }, 
        {
            "location": "/activations/#relu", 
            "text": "tflearn.activations.relu   (x)  Computes rectified linear:  max(features, 0) .", 
            "title": "ReLU"
        }, 
        {
            "location": "/activations/#relu6", 
            "text": "tflearn.activations.relu6   (x)  Computes Rectified Linear 6:  min(max(features, 0), 6) .", 
            "title": "ReLU6"
        }, 
        {
            "location": "/activations/#leakyrelu", 
            "text": "tflearn.activations.leaky_relu   (x,  alpha=0.1,  name='LeakyReLU')  Modified version of ReLU, introducing a nonzero gradient for negative\ninput.", 
            "title": "LeakyReLU"
        }, 
        {
            "location": "/activations/#prelu", 
            "text": "tflearn.activations.prelu   (x,  weights_init='zeros',  name='PReLU')  Parametric Rectified Linear Unit.", 
            "title": "PReLU"
        }, 
        {
            "location": "/activations/#elu", 
            "text": "tflearn.activations.elu   (x)  Exponential Linear Unit.", 
            "title": "ELU"
        }, 
        {
            "location": "/objectives/", 
            "text": "Softmax Categorical Crossentropy\n\n\ntflearn.objectives.softmax_categorical_crossentropy\n  (y_pred,  y_true)\n\n\nComputes softmax cross entropy between y_pred (logits) and\ny_true (labels).\n\n\nMeasures the probability error in discrete classification tasks in which\nthe classes are mutually exclusive (each entry is in exactly one class).\nFor example, each CIFAR-10 image is labeled with one and only one label:\nan image can be a dog or a truck, but not both.\n\n\nWARNING:\n This op expects unscaled logits, since it performs a \nsoftmax\n\non \ny_pred\n internally for efficiency.  Do not call this op with the\noutput of \nsoftmax\n, as it will produce incorrect results.\n\n\ny_pred\n and \ny_true\n must have the same shape \n[batch_size, num_classes]\n\nand the same dtype (either \nfloat32\n or \nfloat64\n). It is also required\nthat \ny_true\n (labels) are binary arrays (For example, class 2 out of a\ntotal of 5 different classes, will be define as [0., 1., 0., 0., 0.])\n\n\nArguments\n\n\n\n\n\ny_pred\n: \nTensor\n. Predicted values.\n\n\ny_true\n: \nTensor\n . Targets (labels), a probability distribution.\n\n\n\n\n\n\nCategorical Crossentropy\n\n\ntflearn.objectives.categorical_crossentropy\n  (y_pred,  y_true)\n\n\nComputes cross entropy between y_pred (logits) and y_true (labels).\n\n\nMeasures the probability error in discrete classification tasks in which\nthe classes are mutually exclusive (each entry is in exactly one class).\nFor example, each CIFAR-10 image is labeled with one and only one label:\nan image can be a dog or a truck, but not both.\n\n\ny_pred\n and \ny_true\n must have the same shape \n[batch_size, num_classes]\n\nand the same dtype (either \nfloat32\n or \nfloat64\n). It is also required\nthat \ny_true\n (labels) are binary arrays (For example, class 2 out of a\ntotal of 5 different classes, will be define as [0., 1., 0., 0., 0.])\n\n\nArguments\n\n\n\n\n\ny_pred\n: \nTensor\n. Predicted values.\n\n\ny_true\n: \nTensor\n . Targets (labels), a probability distribution.\n\n\n\n\n\n\nBinary Crossentropy\n\n\ntflearn.objectives.binary_crossentropy\n  (y_pred,  y_true)\n\n\nComputes sigmoid cross entropy between y_pred (logits) and y_true\n(labels).\n\n\nMeasures the probability error in discrete classification tasks in which\neach class is independent and not mutually exclusive. For instance,\none could perform multilabel classification where a picture can contain\nboth an elephant and a dog at the same time.\n\n\nFor brevity, let \nx = logits\n, \nz = targets\n.  The logistic loss is\n\n\nx - x * z + log(1 + exp(-x))\n\n\nTo ensure stability and avoid overflow, the implementation uses\n\n\nmax(x, 0) - x * z + log(1 + exp(-abs(x)))\n\n\ny_pred\n and \ny_true\n must have the same type and shape.\n\n\nArguments\n\n\n\n\n\ny_pred\n: \nTensor\n of \nfloat\n type. Predicted values.\n\n\ny_true\n: \nTensor\n of \nfloat\n type. Targets (labels).\n\n\n\n\n\n\nMean Square Loss\n\n\ntflearn.objectives.mean_square\n  (y_pred,  y_true)\n\n\nArguments\n\n\n\n\n\ny_pred\n: \nTensor\n of \nfloat\n type. Predicted values.\n\n\ny_true\n: \nTensor\n of \nfloat\n type. Targets (labels).\n\n\n\n\n\n\nHinge Loss\n\n\ntflearn.objectives.hinge_loss\n  (y_pred,  y_true)\n\n\nArguments\n\n\n\n\n\ny_pred\n: \nTensor\n of \nfloat\n type. Predicted values.\n\n\ny_true\n: \nTensor\n of \nfloat\n type. Targets (labels).", 
            "title": "Objectives"
        }, 
        {
            "location": "/objectives/#softmax-categorical-crossentropy", 
            "text": "tflearn.objectives.softmax_categorical_crossentropy   (y_pred,  y_true)  Computes softmax cross entropy between y_pred (logits) and\ny_true (labels).  Measures the probability error in discrete classification tasks in which\nthe classes are mutually exclusive (each entry is in exactly one class).\nFor example, each CIFAR-10 image is labeled with one and only one label:\nan image can be a dog or a truck, but not both.  WARNING:  This op expects unscaled logits, since it performs a  softmax \non  y_pred  internally for efficiency.  Do not call this op with the\noutput of  softmax , as it will produce incorrect results.  y_pred  and  y_true  must have the same shape  [batch_size, num_classes] \nand the same dtype (either  float32  or  float64 ). It is also required\nthat  y_true  (labels) are binary arrays (For example, class 2 out of a\ntotal of 5 different classes, will be define as [0., 1., 0., 0., 0.])", 
            "title": "Softmax Categorical Crossentropy"
        }, 
        {
            "location": "/objectives/#categorical-crossentropy", 
            "text": "tflearn.objectives.categorical_crossentropy   (y_pred,  y_true)  Computes cross entropy between y_pred (logits) and y_true (labels).  Measures the probability error in discrete classification tasks in which\nthe classes are mutually exclusive (each entry is in exactly one class).\nFor example, each CIFAR-10 image is labeled with one and only one label:\nan image can be a dog or a truck, but not both.  y_pred  and  y_true  must have the same shape  [batch_size, num_classes] \nand the same dtype (either  float32  or  float64 ). It is also required\nthat  y_true  (labels) are binary arrays (For example, class 2 out of a\ntotal of 5 different classes, will be define as [0., 1., 0., 0., 0.])", 
            "title": "Categorical Crossentropy"
        }, 
        {
            "location": "/objectives/#binary-crossentropy", 
            "text": "tflearn.objectives.binary_crossentropy   (y_pred,  y_true)  Computes sigmoid cross entropy between y_pred (logits) and y_true\n(labels).  Measures the probability error in discrete classification tasks in which\neach class is independent and not mutually exclusive. For instance,\none could perform multilabel classification where a picture can contain\nboth an elephant and a dog at the same time.  For brevity, let  x = logits ,  z = targets .  The logistic loss is  x - x * z + log(1 + exp(-x))  To ensure stability and avoid overflow, the implementation uses  max(x, 0) - x * z + log(1 + exp(-abs(x)))  y_pred  and  y_true  must have the same type and shape.", 
            "title": "Binary Crossentropy"
        }, 
        {
            "location": "/objectives/#mean-square-loss", 
            "text": "tflearn.objectives.mean_square   (y_pred,  y_true)", 
            "title": "Mean Square Loss"
        }, 
        {
            "location": "/objectives/#hinge-loss", 
            "text": "tflearn.objectives.hinge_loss   (y_pred,  y_true)", 
            "title": "Hinge Loss"
        }, 
        {
            "location": "/optimizers/", 
            "text": "Base Optimizer class\n\n\ntflearn.optimizers.Optimizer\n  (learning_rate,  use_locking,  name)\n\n\nA basic class to create optimizers to be used with TFLearn estimators.\nFirst, The Optimizer class is initialized with given parameters,\nbut no Tensor is created. In a second step, invoking \nget_tensor\n method\nwill actually build the Tensorflow \nOptimizer\n Tensor, and return it.\n\n\nThis way, a user can easily specifies an optimizer with non default\nparameters and learning rate decay, while TFLearn estimators will\nbuild the optimizer and a step tensor by itself.\n\n\nArguments\n\n\n\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\nuse_locking\n: \nbool\n. If True use locks for update operation.\n\n\nname\n: \nstr\n. The optimizer name.\n\n\n\n\nAttributes\n\n\n\n\n\ntensor\n: \nOptimizer\n. The optimizer tensor.\n\n\nhas_decay\n: \nbool\n. True if optimizer has a learning rate decay.\n\n\n\n\nMethods\n\n\n\n \n\n\nbuild\n  (step_tensor=None)\n\n\nThis method creates the optimizer with specified parameters. It must\nbe implemented for every \nOptimizer\n.\n\n\nArguments\n\n\n\n\n\nstep_tensor\n: \ntf.Tensor\n. A variable holding the training step.\nOnly necessary when optimizer has a learning rate decay.\n\n\n\n\n \n\n\nget_tensor\n  (self)\n\n\nA method to retrieve the optimizer tensor.\n\n\nReturns\n\n\n\nThe \nOptimizer\n.\n\n\n\n\nStochastic Gradient Descent\n\n\ntflearn.optimizers.SGD\n  (learning_rate=0.001,  lr_decay=0.0,  decay_step=100,  staircase=False,  use_locking=False,  name='SGD')\n\n\nSGD Optimizer accepts learning rate decay. When training a model,\nit is often recommended to lower the learning rate as the training\nprogresses. The function returns the decayed learning rate.  It is\ncomputed as:\n\n\ndecayed_learning_rate = learning_rate *  decay_rate ^ (global_step / decay_steps)\n\n\n\n\nExamples\n\n\n\n# With TFLearn estimators.\nsgd = SGD(learning_rate=0.01, lr_decay=0.96, decay_step=100)\nregression = regression(net, optimizer=sgd)\n\n# Without TFLearn estimators (returns tf.Optimizer).\nsgd = SGD(learning_rate=0.01).get_tensor()\n\n\n\n\nArguments\n\n\n\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\nuse_locking\n: \nbool\n. If True use locks for update operation.\n\n\nlr_decay\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_step\n: \nint\n. Apply decay every provided steps.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nuse_locking\n: \nbool\n. If True use locks for update operation.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when\napplying gradients. Defaults to \"GradientDescent\".\n\n\n\n\n\n\nRMSprop\n\n\ntflearn.optimizers.RMSProp\n  (learning_rate=0.001,  decay=0.9,  momentum=0.0,  epsilon=1e-10,  use_locking=False,  name='RMSProp')\n\n\nMaintain a moving (discounted) average of the square of gradients.\nDivide gradient by the root of this average.\n\n\nExamples\n\n\n\n# With TFLearn estimators.\nrmsprop = RMSProp(learning_rate=0.1, decay=0.999)\nregression = regression(net, optimizer=rmsprop)\n\n# Without TFLearn estimators (returns tf.Optimizer).\nrmsprop = RMSProp(learning_rate=0.01, decay=0.999).get_tensor()\n# or\nrmsprop = RMSProp(learning_rate=0.01, decay=0.999)()\n\n\n\n\n\nArguments\n\n\n\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\ndecay\n: \nfloat\n. Discounting factor for the history/coming gradient.\n\n\nmomentum\n: \nfloat\n. Momentum.\n\n\nepsilon\n: \nfloat\n. Small value to avoid zero denominator.\n\n\nuse_locking\n: \nbool\n. If True use locks for update operation.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when\napplying gradients. Defaults to \"RMSProp\".\n\n\n\n\n\n\nAdam\n\n\ntflearn.optimizers.Adam\n  (learning_rate=0.001,  beta1=0.9,  beta2=0.999,  epsilon=1e-08,  use_locking=False,  name='Adam')\n\n\nThe default value of 1e-8 for epsilon might not be a good default in\ngeneral. For example, when training an Inception network on ImageNet a\ncurrent good choice is 1.0 or 0.1.\n\n\nExamples\n\n\n\n# With TFLearn estimators\nadam = Adam(learning_rate=0.001, beta1=0.99)\nregression = regression(net, optimizer=adam)\n\n# Without TFLearn estimators (returns tf.Optimizer)\nadam = Adam(learning_rate=0.01).get_tensor()\n\n\n\n\n\nArguments\n\n\n\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\nbeta1\n: \nfloat\n. The exponential decay rate for the 1st moment\nestimates.\n\n\nbeta2\n: \nfloat\n. The exponential decay rate for the 2nd moment\nestimates.\n\n\nepsilon\n: \nfloat\n. A small constant for numerical stability.\n\n\nuse_locking\n: \nbool\n. If True use locks for update operation.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when\napplying gradients. Defaults to \"Adam\".\n\n\n\n\nReferences\n\n\n\nAdam: A Method for Stochastic Optimization. Diederik Kingma,\nJimmy Ba. ICLR 2015.\n\n\nLinks\n\n\n\nPaper\n\n\n\n\nMomentum\n\n\ntflearn.optimizers.Momentum\n  (learning_rate=0.001,  momentum=0.9,  lr_decay=0.0,  decay_step=100,  staircase=False,  use_locking=False,  name='Momentum')\n\n\nMomentum Optimizer accepts learning rate decay. When training a model,\nit is often recommended to lower the learning rate as the training\nprogresses. The function returns the decayed learning rate.  It is\ncomputed as:\n\n\ndecayed_learning_rate = learning_rate *  decay_rate ^ (global_step / decay_steps)\n\n\n\n\nExamples\n\n\n\n# With TFLearn estimators\nmomentum = Momentum(learning_rate=0.01, lr_decay=0.96, decay_step=100)\nregression = regression(net, optimizer=momentum)\n\n# Without TFLearn estimators (returns tf.Optimizer)\nmm = Momentum(learning_rate=0.01, lr_decay=0.96).get_tensor()\n\n\n\n\nArguments\n\n\n\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\nmomentum\n: \nfloat\n. Momentum.\n\n\nlr_decay\n: \nfloat\n. The learning rate decay to apply.\n\n\ndecay_step\n: \nint\n. Apply decay every provided steps.\n\n\nstaircase\n: \nbool\n. It \nTrue\n decay learning rate at discrete intervals.\n\n\nuse_locking\n: \nbool\n. If True use locks for update operation.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when\napplying gradients. Defaults to \"Momentum\".\n\n\n\n\n\n\nAdaGrad\n\n\ntflearn.optimizers.AdaGrad\n  (learning_rate=0.001,  initial_accumulator_value=0.1,  use_locking=False,  name='AdaGrad')\n\n\nExamples\n\n\n\n# With TFLearn estimators\nadagrad = AdaGrad(learning_rate=0.01, initial_accumulator_value=0.01)\nregression = regression(net, optimizer=adagrad)\n\n# Without TFLearn estimators (returns tf.Optimizer)\nadagrad = AdaGrad(learning_rate=0.01).get_tensor()\n\n\n\n\nArguments\n\n\n\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\ninitial_accumulator_value\n: \nfloat\n. Starting value for the\naccumulators, must be positive\n\n\nuse_locking\n: \nbool\n. If True use locks for update operation.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when\napplying gradients. Defaults to \"AdaGrad\".\n\n\n\n\nReferences\n\n\n\nAdaptive Subgradient Methods for Online Learning and Stochastic\nOptimization. J. Duchi, E. Hazan \n Y. Singer. Journal of Machine\nLearning Research 12 (2011) 2121-2159.\n\n\nLinks\n\n\n\nPaper\n\n\n\n\nFtrl Proximal\n\n\ntflearn.optimizers.Ftrl\n  (learning_rate=3.0,  learning_rate_power=-0.5,  initial_accumulator_value=0.1,  l1_regularization_strength=0.0,  l2_regularization_strength=0.0,  use_locking=False,  name='Ftrl')\n\n\nThe Ftrl-proximal algorithm, abbreviated for Follow-the-regularized-leader,\nis described in the paper below.\n\n\nIt can give a good performance vs. sparsity tradeoff.\n\n\nFtrl-proximal uses its own global base learning rate and can behave like\nAdagrad with \nlearning_rate_power=-0.5\n, or like gradient descent with\n\nlearning_rate_power=0.0\n.\n\n\nExamples\n\n\n\n# With TFLearn estimators.\nftrl = Ftrl(learning_rate=0.01, learning_rate_power=-0.1)\nregression = regression(net, optimizer=ftrl)\n\n# Without TFLearn estimators (returns tf.Optimizer).\nftrl = Ftrl(learning_rate=0.01).get_tensor()\n\n\n\n\nArguments\n\n\n\n\n\nlearning_rate\n: \nfloat\n. Learning rate.\n\n\nlearning_rate_power\n: \nfloat\n. Must be less or equal to zero.\n\n\ninitial_accumulator_value\n: \nfloat\n. The starting value for accumulators.\nOnly positive values are allowed.\n\n\nl1_regularization_strength\n: \nfloat\n. Must be less or equal to zero.\n\n\nl2_regularization_strength\n: \nfloat\n. Must be less or equal to zero.\n\n\nuse_locking\n: bool`. If True use locks for update operation.\n\n\nname\n: \nstr\n. Optional name prefix for the operations created when\napplying gradients. Defaults to \"Ftrl\".\n\n\n\n\nLinks\n\n\n\nAd Click Prediction: a View from the Trenches", 
            "title": "Optimizers"
        }, 
        {
            "location": "/optimizers/#base-optimizer-class", 
            "text": "tflearn.optimizers.Optimizer   (learning_rate,  use_locking,  name)  A basic class to create optimizers to be used with TFLearn estimators.\nFirst, The Optimizer class is initialized with given parameters,\nbut no Tensor is created. In a second step, invoking  get_tensor  method\nwill actually build the Tensorflow  Optimizer  Tensor, and return it.  This way, a user can easily specifies an optimizer with non default\nparameters and learning rate decay, while TFLearn estimators will\nbuild the optimizer and a step tensor by itself.", 
            "title": "Base Optimizer class"
        }, 
        {
            "location": "/optimizers/#stochastic-gradient-descent", 
            "text": "tflearn.optimizers.SGD   (learning_rate=0.001,  lr_decay=0.0,  decay_step=100,  staircase=False,  use_locking=False,  name='SGD')  SGD Optimizer accepts learning rate decay. When training a model,\nit is often recommended to lower the learning rate as the training\nprogresses. The function returns the decayed learning rate.  It is\ncomputed as:  decayed_learning_rate = learning_rate *  decay_rate ^ (global_step / decay_steps)", 
            "title": "Stochastic Gradient Descent"
        }, 
        {
            "location": "/optimizers/#rmsprop", 
            "text": "tflearn.optimizers.RMSProp   (learning_rate=0.001,  decay=0.9,  momentum=0.0,  epsilon=1e-10,  use_locking=False,  name='RMSProp')  Maintain a moving (discounted) average of the square of gradients.\nDivide gradient by the root of this average.", 
            "title": "RMSprop"
        }, 
        {
            "location": "/optimizers/#adam", 
            "text": "tflearn.optimizers.Adam   (learning_rate=0.001,  beta1=0.9,  beta2=0.999,  epsilon=1e-08,  use_locking=False,  name='Adam')  The default value of 1e-8 for epsilon might not be a good default in\ngeneral. For example, when training an Inception network on ImageNet a\ncurrent good choice is 1.0 or 0.1.", 
            "title": "Adam"
        }, 
        {
            "location": "/optimizers/#momentum", 
            "text": "tflearn.optimizers.Momentum   (learning_rate=0.001,  momentum=0.9,  lr_decay=0.0,  decay_step=100,  staircase=False,  use_locking=False,  name='Momentum')  Momentum Optimizer accepts learning rate decay. When training a model,\nit is often recommended to lower the learning rate as the training\nprogresses. The function returns the decayed learning rate.  It is\ncomputed as:  decayed_learning_rate = learning_rate *  decay_rate ^ (global_step / decay_steps)", 
            "title": "Momentum"
        }, 
        {
            "location": "/optimizers/#adagrad", 
            "text": "tflearn.optimizers.AdaGrad   (learning_rate=0.001,  initial_accumulator_value=0.1,  use_locking=False,  name='AdaGrad')", 
            "title": "AdaGrad"
        }, 
        {
            "location": "/optimizers/#ftrl-proximal", 
            "text": "tflearn.optimizers.Ftrl   (learning_rate=3.0,  learning_rate_power=-0.5,  initial_accumulator_value=0.1,  l1_regularization_strength=0.0,  l2_regularization_strength=0.0,  use_locking=False,  name='Ftrl')  The Ftrl-proximal algorithm, abbreviated for Follow-the-regularized-leader,\nis described in the paper below.  It can give a good performance vs. sparsity tradeoff.  Ftrl-proximal uses its own global base learning rate and can behave like\nAdagrad with  learning_rate_power=-0.5 , or like gradient descent with learning_rate_power=0.0 .", 
            "title": "Ftrl Proximal"
        }, 
        {
            "location": "/metrics/", 
            "text": "Base Metric Class\n\n\ntflearn.metrics.Metric\n  (name=None)\n\n\nMetric class is meant to be used by TFLearn models class. It can be\nfirst initialized with desired parameters, and a model class will\nbuild it later using the given network output and targets.\n\n\nAttributes\n\n\n\n\n\ntensor\n: \nTensor\n. The metric tensor.\n\n\n\n\nMethods\n\n\n\n \n\n\nbuild\n  (predictions,  targets,  inputs)\n\n\nBuild metric method, with common arguments to all Metrics.\n\n\nArguments\n\n\n\n\n\nprediction\n: \nTensor\n. The network to perform prediction.\n\n\ntargets\n: \nTensor\n. The targets (labels).\n\n\ninputs\n: \nTensor\n. The input data.\n\n\n\n\n \n\n\nget_tensor\n  (self)\n\n\nGet the metric tensor.\n\n\nReturns\n\n\n\nThe metric \nTensor\n.\n\n\n\n\nAccuracy\n\n\ntflearn.metrics.Accuracy\n  (name='acc')\n\n\nComputes the model accuracy.\n\n\nExamples\n\n\n\n# To be used with TFLearn estimators\nacc = Accuracy()\nregression = regression(net, metric=acc)\n\n\n\n\nArguments\n\n\n\n\n\nname\n: The name to display.\n\n\n\n\n\n\nTop-k\n\n\ntflearn.metrics.Top_k\n  (k=1,  name=None)\n\n\nComputes Top-k mean accuracy (whether the targets are in the top 'K'\npredictions).\n\n\nExamples\n\n\n\n# To be used with TFLearn estimators\ntop5 = Top_k(k=5)\nregression = regression(net, metric=top5)\n\n\n\n\nArguments\n\n\n\n\n\nk\n: \nint\n. Number of top elements to look at for computing precision.\n\n\nname\n: The name to display.\n\n\n\n\n\n\nStandard Error\n\n\ntflearn.metrics.R2\n  (name=None)\n\n\nComputes coefficient of determination. Useful to evaluate a linear\nregression.\n\n\nExamples\n\n\n\n# To be used with TFLearn estimators\nr2 = R2()\nregression = regression(net, metric=r2)\n\n\n\n\nArguments\n\n\n\n\n\nname\n: The name to display.\n\n\n\n\n\n\naccuracy_op\n\n\ntflearn.metrics.accuracy_op\n  (predictions,  targets)\n\n\nAn op that calculates mean accuracy.\n\n\nExamples\n\n\n\ninput_data = placeholder(shape=[None, 784])\ny_pred = my_network(input_data) # Apply some ops\ny_true = placeholder(shape=[None, 10]) # Labels\nacc_op = accuracy_op(y_pred, y_true)\n\n# Calculate accuracy by feeding data X and labels Y\naccuracy = sess.run(acc_op, feed_dict={input_data: X, y_true: Y})\n\n\n\n\nArguments\n\n\n\n\n\npredictions\n: \nTensor\n.\n\n\ntargets\n: \nTensor\n.\n\n\n\n\nReturns\n\n\n\nFloat\n. The mean accuracy.\n\n\n\n\ntop_k_op\n\n\ntflearn.metrics.top_k_op\n  (predictions,  targets,  k=1)\n\n\nAn op that calculates top-k mean accuracy.\n\n\nExamples\n\n\n\ninput_data = placeholder(shape=[None, 784])\ny_pred = my_network(input_data) # Apply some ops\ny_true = placeholder(shape=[None, 10]) # Labels\ntop3_op = top_k_op(y_pred, y_true, 3)\n\n# Calculate Top-3 accuracy by feeding data X and labels Y\ntop3_accuracy = sess.run(top3_op, feed_dict={input_data: X, y_true: Y})\n\n\n\n\nArguments\n\n\n\n\n\npredictions\n: \nTensor\n.\n\n\ntargets\n: \nTensor\n.\n\n\nk\n: \nint\n. Number of top elements to look at for computing precision.\n\n\n\n\nReturns\n\n\n\nFloat\n. The top-k mean accuracy.\n\n\n\n\nr2_op\n\n\ntflearn.metrics.r2_op\n  (predictions,  targets,  inputs)\n\n\nAn op that calculates the standard error.\n\n\nExamples\n\n\n\ninput_data = placeholder(shape=[None, 784])\ny_pred = my_network(input_data) # Apply some ops\ny_true = placeholder(shape=[None, 10]) # Labels\nstderr_op = r2_op(y_pred, y_true, input_data)\n\n# Calculate standard error by feeding data X and labels Y\nstd_error = sess.run(stderr_op, feed_dict={input_data: X, y_true: Y})\n\n\n\n\nArguments\n\n\n\n\n\npredictions\n: \nTensor\n.\n\n\ntargets\n: \nTensor\n.\n\n\ninputs\n: \nTensor\n.\n\n\n\n\nReturns\n\n\n\nFloat\n. The standard error.", 
            "title": "Metrics"
        }, 
        {
            "location": "/metrics/#base-metric-class", 
            "text": "tflearn.metrics.Metric   (name=None)  Metric class is meant to be used by TFLearn models class. It can be\nfirst initialized with desired parameters, and a model class will\nbuild it later using the given network output and targets.", 
            "title": "Base Metric Class"
        }, 
        {
            "location": "/metrics/#accuracy", 
            "text": "tflearn.metrics.Accuracy   (name='acc')  Computes the model accuracy.", 
            "title": "Accuracy"
        }, 
        {
            "location": "/metrics/#top-k", 
            "text": "tflearn.metrics.Top_k   (k=1,  name=None)  Computes Top-k mean accuracy (whether the targets are in the top 'K'\npredictions).", 
            "title": "Top-k"
        }, 
        {
            "location": "/metrics/#standard-error", 
            "text": "tflearn.metrics.R2   (name=None)  Computes coefficient of determination. Useful to evaluate a linear\nregression.", 
            "title": "Standard Error"
        }, 
        {
            "location": "/metrics/#accuracy_op", 
            "text": "tflearn.metrics.accuracy_op   (predictions,  targets)  An op that calculates mean accuracy.", 
            "title": "accuracy_op"
        }, 
        {
            "location": "/metrics/#top_k_op", 
            "text": "tflearn.metrics.top_k_op   (predictions,  targets,  k=1)  An op that calculates top-k mean accuracy.", 
            "title": "top_k_op"
        }, 
        {
            "location": "/metrics/#r2_op", 
            "text": "tflearn.metrics.r2_op   (predictions,  targets,  inputs)  An op that calculates the standard error.", 
            "title": "r2_op"
        }, 
        {
            "location": "/initializations/", 
            "text": "Zeros\n\n\ntflearn.initializations.zeros\n  (shape=None,  dtype=tf.float32,  seed=None)\n\n\nInitialize a tensor with all elements set to zero.\n\n\nArguments\n\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\ndtype\n: The tensor data type.\n\n\n\n\nReturns\n\n\n\nThe Initializer, or an initialized \nTensor\n if a shape is specified.\n\n\n\n\nUniform\n\n\ntflearn.initializations.uniform\n  (shape=None,  minval=0,  maxval=None,  dtype=tf.float32,  seed=None)\n\n\nInitialization with random values from a uniform distribution.\n\n\nThe generated values follow a uniform distribution in the range\n\n[minval, maxval)\n. The lower bound \nminval\n is included in the range,\nwhile the upper bound \nmaxval\n is excluded.\n\n\nFor floats, the default range is \n[0, 1)\n.  For ints, at least \nmaxval\n\nmust be specified explicitly.\n\n\nIn the integer case, the random integers are slightly biased unless\n\nmaxval - minval\n is an exact power of two.  The bias is small for values of\n\nmaxval - minval\n significantly smaller than the range of the output (either\n\n2**32\n or \n2**64\n).\n\n\nArguments\n\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\ndtype\n: The tensor data type. Only float are supported.\n\n\nseed\n: \nint\n. Used to create a random seed for the distribution.\n\n\n\n\nReturns\n\n\n\nThe Initializer, or an initialized \nTensor\n if shape is specified.\n\n\n\n\nUniform Scaling\n\n\ntflearn.initializations.uniform_scaling\n  (shape=None,  factor=1.0,  dtype=tf.float32,  seed=None)\n\n\nInitialization with random values from uniform distribution without scaling\nvariance.\n\n\nWhen initializing a deep network, it is in principle advantageous to keep\nthe scale of the input variance constant, so it does not explode or diminish\nby reaching the final layer. If the input is \nx\n and the operation \nx * W\n,\nand we want to initialize \nW\n uniformly at random, we need to pick \nW\n from\n\n\n[-sqrt(3) / sqrt(dim), sqrt(3) / sqrt(dim)]\n\n\nto keep the scale intact, where \ndim = W.shape[0]\n (the size of the input).\nA similar calculation for convolutional networks gives an analogous result\nwith \ndim\n equal to the product of the first 3 dimensions.  When\nnonlinearities are present, we need to multiply this by a constant \nfactor\n.\nSee \nSussillo et al., 2014\n\n(\npdf\n) for deeper motivation, experiments\nand the calculation of constants. In section 2.3 there, the constants were\nnumerically computed: for a linear layer it's 1.0, relu: ~1.43, tanh: ~1.15.\n\n\nArguments\n\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\nfactor\n: \nfloat\n. A multiplicative factor by which the values will be\nscaled.\n\n\ndtype\n: The tensor data type. Only float are supported.\n\n\nseed\n: \nint\n. Used to create a random seed for the distribution.\n\n\n\n\nReturns\n\n\n\nThe Initializer, or an initialized \nTensor\n if shape is specified.\n\n\n\n\nNormal\n\n\ntflearn.initializations.normal\n  (shape=None,  mean=0.0,  stddev=0.02,  dtype=tf.float32,  seed=None)\n\n\nInitialization with random values from a normal distribution.\n\n\nArguments\n\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\nmean\n: Same as \ndtype\n. The mean of the truncated normal distribution.\n\n\nstddev\n: Same as \ndtype\n. The standard deviation of the truncated\nnormal distribution.\n\n\ndtype\n: The tensor data type.\n\n\nseed\n: \nint\n. Used to create a random seed for the distribution.\n\n\n\n\nReturns\n\n\n\nThe Initializer, or an initialized \nTensor\n if shape is specified.\n\n\n\n\nTruncated Normal\n\n\ntflearn.initializations.truncated_normal\n  (shape=None,  mean=0.0,  stddev=0.02,  dtype=tf.float32,  seed=None)\n\n\nInitialization with random values from a normal truncated distribution.\n\n\nThe generated values follow a normal distribution with specified mean and\nstandard deviation, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.\n\n\nArguments\n\n\n\n\n\nshape\n: List of \nint\n. A shape to initialize a Tensor (optional).\n\n\nmean\n: Same as \ndtype\n. The mean of the truncated normal distribution.\n\n\nstddev\n: Same as \ndtype\n. The standard deviation of the truncated\nnormal distribution.\n\n\ndtype\n: The tensor data type.\n\n\nseed\n: \nint\n. Used to create a random seed for the distribution.\n\n\n\n\nReturns\n\n\n\nThe Initializer, or an initialized \nTensor\n if shape is specified.", 
            "title": "Initializations"
        }, 
        {
            "location": "/initializations/#zeros", 
            "text": "tflearn.initializations.zeros   (shape=None,  dtype=tf.float32,  seed=None)  Initialize a tensor with all elements set to zero.", 
            "title": "Zeros"
        }, 
        {
            "location": "/initializations/#uniform", 
            "text": "tflearn.initializations.uniform   (shape=None,  minval=0,  maxval=None,  dtype=tf.float32,  seed=None)  Initialization with random values from a uniform distribution.  The generated values follow a uniform distribution in the range [minval, maxval) . The lower bound  minval  is included in the range,\nwhile the upper bound  maxval  is excluded.  For floats, the default range is  [0, 1) .  For ints, at least  maxval \nmust be specified explicitly.  In the integer case, the random integers are slightly biased unless maxval - minval  is an exact power of two.  The bias is small for values of maxval - minval  significantly smaller than the range of the output (either 2**32  or  2**64 ).", 
            "title": "Uniform"
        }, 
        {
            "location": "/initializations/#uniform-scaling", 
            "text": "tflearn.initializations.uniform_scaling   (shape=None,  factor=1.0,  dtype=tf.float32,  seed=None)  Initialization with random values from uniform distribution without scaling\nvariance.  When initializing a deep network, it is in principle advantageous to keep\nthe scale of the input variance constant, so it does not explode or diminish\nby reaching the final layer. If the input is  x  and the operation  x * W ,\nand we want to initialize  W  uniformly at random, we need to pick  W  from  [-sqrt(3) / sqrt(dim), sqrt(3) / sqrt(dim)]  to keep the scale intact, where  dim = W.shape[0]  (the size of the input).\nA similar calculation for convolutional networks gives an analogous result\nwith  dim  equal to the product of the first 3 dimensions.  When\nnonlinearities are present, we need to multiply this by a constant  factor .\nSee  Sussillo et al., 2014 \n( pdf ) for deeper motivation, experiments\nand the calculation of constants. In section 2.3 there, the constants were\nnumerically computed: for a linear layer it's 1.0, relu: ~1.43, tanh: ~1.15.", 
            "title": "Uniform Scaling"
        }, 
        {
            "location": "/initializations/#normal", 
            "text": "tflearn.initializations.normal   (shape=None,  mean=0.0,  stddev=0.02,  dtype=tf.float32,  seed=None)  Initialization with random values from a normal distribution.", 
            "title": "Normal"
        }, 
        {
            "location": "/initializations/#truncated-normal", 
            "text": "tflearn.initializations.truncated_normal   (shape=None,  mean=0.0,  stddev=0.02,  dtype=tf.float32,  seed=None)  Initialization with random values from a normal truncated distribution.  The generated values follow a normal distribution with specified mean and\nstandard deviation, except that values whose magnitude is more than 2 standard\ndeviations from the mean are dropped and re-picked.", 
            "title": "Truncated Normal"
        }, 
        {
            "location": "/losses/", 
            "text": "L2\n\n\ntflearn.losses.L2\n  (tensor,  wd=0.001)\n\n\nComputes half the L2 norm of a tensor without the \nsqrt\n:\n\n\noutput = sum(t ** 2) / 2 * wd\n\n\nArguments\n\n\n\n\n\ntensor\n: \nTensor\n. The tensor to apply regularization.\n\n\nwd\n: \nfloat\n. The decay.\n\n\n\n\nReturns\n\n\n\nThe regularization \nTensor\n.\n\n\n\n\nL1\n\n\ntflearn.losses.L1\n  (tensor,  wd=0.001)\n\n\nComputes the L1 norm of a tensor:\n\n\noutput = sum(|t|) * wd\n\n\nArguments\n\n\n\n\n\ntensor\n: \nTensor\n. The tensor to apply regularization.\n\n\nwd\n: \nfloat\n. The decay.\n\n\n\n\nReturns\n\n\n\nThe regularization \nTensor\n.", 
            "title": "Losses"
        }, 
        {
            "location": "/losses/#l2", 
            "text": "tflearn.losses.L2   (tensor,  wd=0.001)  Computes half the L2 norm of a tensor without the  sqrt :  output = sum(t ** 2) / 2 * wd", 
            "title": "L2"
        }, 
        {
            "location": "/losses/#l1", 
            "text": "tflearn.losses.L1   (tensor,  wd=0.001)  Computes the L1 norm of a tensor:  output = sum(|t|) * wd", 
            "title": "L1"
        }, 
        {
            "location": "/summaries/", 
            "text": "get_summary\n\n\ntflearn.summaries.get_summary\n  (stype,  tag,  value=None,  collection_key=None,  break_if_exists=False)\n\n\nCreate or retrieve a summary. It keep tracks of all graph summaries\nthrough summary_tags collection. If a summary tags already exists,\nit will return that summary tensor or raise an error (according to\n'break_if_exists').\n\n\nArguments\n\n\n\n\n\nstype\n: \nstr\n. Summary type: 'histogram', 'scalar' or 'image'.\n\n\ntag\n: \nstr\n. The summary tag (name).\n\n\nvalue\n: \nTensor\n. The summary initialization value. Default: None.\n\n\ncollection_key\n: \nstr\n. If specified, the created summary will be\nadded to that collection (optional).\n\n\nbreak_if_exists\n: \nbool\n. If True, if a summary with same tag already\nexists, it will raise an exception (instead of returning that\nexisting summary).\n\n\n\n\nReturns\n\n\n\nThe summary \nTensor\n.\n\n\n\n\nadd_activations_summary\n\n\ntflearn.summaries.add_activations_summary\n  (activation_ops,  name_prefix='',  name_suffix='',  collection_key=None)\n\n\nAdd histogram summary for given activations.\n\n\nArguments\n\n\n\n\n\nactivation_ops\n: A list of \nTensor\n. The activations to summarize.\n\n\nname_prefix\n: \nstr\n. A prefix to add to summary scope.\n\n\nname_suffix\n: \nstr\n. A suffix to add to summary scope.\n\n\ncollection_key\n: \nstr\n. A collection to store the summaries.\n\n\n\n\nReturns\n\n\n\nThe list of created activation summaries.\n\n\n\n\nadd_gradients_summary\n\n\ntflearn.summaries.add_gradients_summary\n  (grads,  name_prefix='',  name_suffix='',  collection_key=None)\n\n\nAdd histogram summary for given gradients.\n\n\nArguments\n\n\n\n\n\ngrads\n: A list of \nTensor\n. The gradients to summarize.\n\n\nname_prefix\n: \nstr\n. A prefix to add to summary scope.\n\n\nname_suffix\n: \nstr\n. A suffix to add to summary scope.\n\n\ncollection_key\n: \nstr\n. A collection to store the summaries.\n\n\n\n\nReturns\n\n\n\nThe list of created gradient summaries.\n\n\n\n\nadd_trainable_vars_summary\n\n\ntflearn.summaries.add_trainable_vars_summary\n  (variables,  name_prefix='',  name_suffix='',  collection_key=None)\n\n\nAdd histogram summary for given variables weights.\n\n\nArguments\n\n\n\n\n\nvariables\n: A list of \nVariable\n. The variables to summarize.\n\n\nname_prefix\n: \nstr\n. A prefix to add to summary scope.\n\n\nname_suffix\n: \nstr\n. A suffix to add to summary scope.\n\n\ncollection_key\n: \nstr\n. A collection to store the summaries.\n\n\n\n\nReturns\n\n\n\nThe list of created weights summaries.\n\n\n\n\nget_value_from_summary_string\n\n\ntflearn.summaries.get_value_from_summary_string\n  (tag,  summary_str)\n\n\nRetrieve a summary value from a summary string.\n\n\nArguments\n\n\n\n\n\ntag\n: \nstr\n. The summary tag (name).\n\n\nsummary_str\n: \nstr\n. The summary string to look in.\n\n\n\n\nReturns\n\n\n\nA \nfloat\n. The retrieved value.\n\n\n\n\nadd_loss_summaries\n\n\ntflearn.summaries.add_loss_summaries\n  (total_loss,  loss,  regul_losses_collection_key,  name_prefix='',  summaries_collection_key=None,  exp_moving_avg=0.9,  ema_num_updates=None)\n\n\nAdd scalar summaries (raw and averages) for given losses.\n\n\nGenerates moving average for all losses and associated summaries for\nvisualizing the performance of the network.\n\n\nArguments\n\n\n\n\n\ntotal_loss\n: \nTensor\n. The total loss (Regression loss +\nregularization losses).\n\n\nloss\n: \nTensor\n. Regression loss.\n\n\nname_prefix\n: \nstr\n. A prefix to add to the summary name.\n\n\nregul_losses_collection_key\n: \nstr\n. A collection name to retrieve\nregularization losses.\n\n\nexp_moving_avg\n: \nfloat\n. Exponential moving average.\n\n\nema_num_updates\n: \nint\n. Step to be used with exp moving avg.\n\n\n\n\nReturns\n\n\n\nloss_averages_op: op for generating moving averages of losses.\n\n\n\n\nsummary_exists\n\n\ntflearn.summaries.summary_exists\n  (tag)\n\n\nCheck if a summary exists.\n\n\nArguments\n\n\n\n\n\ntag\n: \nstr\n. The summary name.\n\n\n\n\nReturns\n\n\n\nA \nbool\n. Whether the summary exists or not.", 
            "title": "Summaries"
        }, 
        {
            "location": "/summaries/#get_summary", 
            "text": "tflearn.summaries.get_summary   (stype,  tag,  value=None,  collection_key=None,  break_if_exists=False)  Create or retrieve a summary. It keep tracks of all graph summaries\nthrough summary_tags collection. If a summary tags already exists,\nit will return that summary tensor or raise an error (according to\n'break_if_exists').", 
            "title": "get_summary"
        }, 
        {
            "location": "/summaries/#add_activations_summary", 
            "text": "tflearn.summaries.add_activations_summary   (activation_ops,  name_prefix='',  name_suffix='',  collection_key=None)  Add histogram summary for given activations.", 
            "title": "add_activations_summary"
        }, 
        {
            "location": "/summaries/#add_gradients_summary", 
            "text": "tflearn.summaries.add_gradients_summary   (grads,  name_prefix='',  name_suffix='',  collection_key=None)  Add histogram summary for given gradients.", 
            "title": "add_gradients_summary"
        }, 
        {
            "location": "/summaries/#add_trainable_vars_summary", 
            "text": "tflearn.summaries.add_trainable_vars_summary   (variables,  name_prefix='',  name_suffix='',  collection_key=None)  Add histogram summary for given variables weights.", 
            "title": "add_trainable_vars_summary"
        }, 
        {
            "location": "/summaries/#get_value_from_summary_string", 
            "text": "tflearn.summaries.get_value_from_summary_string   (tag,  summary_str)  Retrieve a summary value from a summary string.", 
            "title": "get_value_from_summary_string"
        }, 
        {
            "location": "/summaries/#add_loss_summaries", 
            "text": "tflearn.summaries.add_loss_summaries   (total_loss,  loss,  regul_losses_collection_key,  name_prefix='',  summaries_collection_key=None,  exp_moving_avg=0.9,  ema_num_updates=None)  Add scalar summaries (raw and averages) for given losses.  Generates moving average for all losses and associated summaries for\nvisualizing the performance of the network.", 
            "title": "add_loss_summaries"
        }, 
        {
            "location": "/summaries/#summary_exists", 
            "text": "tflearn.summaries.summary_exists   (tag)  Check if a summary exists.", 
            "title": "summary_exists"
        }, 
        {
            "location": "/variables/", 
            "text": "variable\n\n\ntflearn.variables.variable\n  (name,  shape=None,  dtype=tf.float32,  initializer=None,  regularizer=None,  trainable=True,  collections=None,  device='',  restore=True)\n\n\nInstantiate a new variable.\n\n\nArguments\n\n\n\n\n\nname\n: \nstr\n. A name for this variable.\n\n\nshape\n: list of \nint\n. The variable shape (optional).\n\n\ndtype\n: \ntype\n. The variable data type.\n\n\ninitializer\n: \nstr\n or \nTensor\n. The variable initialization. (See\ntflearn.initializations for references).\n\n\nregularizer\n: \nstr\n or \nTensor\n. The variable regularizer. (See\ntflearn.losses for references).\n\n\ntrainable\n: \nbool\n. If True, this variable weights will be trained.\n\n\ncollections\n: \nstr\n. A collection to add the new variable to (optional).\n\n\ndevice\n: \nstr\n. Device ID to store the variable. Default: '/cpu:0'.\n\n\nrestore\n: \nbool\n. Restore or not this variable when loading a\npre-trained model (Only compatible with tflearn pre-built\ntraining functions).\n\n\n\n\nReturns\n\n\n\nA Variable.\n\n\n\n\nget_all_variables\n\n\ntflearn.variables.get_all_variables\n  ()\n\n\nGet all Graph variables.\n\n\nReturns\n\n\n\nA list of Variables.\n\n\n\n\nget_all_variables\n\n\ntflearn.variables.get_all_trainable_variable\n  ()\n\n\nGet all Graph trainable variables.\n\n\nReturns\n\n\n\nA list of Variables.\n\n\n\n\nget_layer_variables_by_name\n\n\ntflearn.variables.get_layer_variables_by_name\n  (name)\n\n\nRetrieve a layer's variables, given its name.\n\n\nArguments\n\n\n\n\n\nname\n: \nstr\n. The layer name.\n\n\n\n\nReturns\n\n\n\nA list of Variables.\n\n\n\n\nget_value\n\n\ntflearn.variables.get_value\n  (var,  session=None)\n\n\nGet a variable's value. If no session provided, use default one.\n\n\nArguments\n\n\n\n\n\nvar\n: \nVariable\n. The variable to get value from.\n\n\nsession\n: \nSession\n. The session to run the op. Default: the default\nsession.\n\n\n\n\nReturns\n\n\n\nThe variable's value.\n\n\n\n\nget_value\n\n\ntflearn.variables.set_value\n  (var,  value,  session=None)\n\n\nSet a variable's value. If no session provided, use default one.\n\n\nArguments\n\n\n\n\n\nvar\n: \nVariable\n. The variable to assign a value.\n\n\nvalue\n: The value to assign. Must be compatible with variable dtype.\n\n\nsession\n: \nSession\n. The session to perform the assignation.\nDefault: the default session.", 
            "title": "Variables"
        }, 
        {
            "location": "/variables/#variable", 
            "text": "tflearn.variables.variable   (name,  shape=None,  dtype=tf.float32,  initializer=None,  regularizer=None,  trainable=True,  collections=None,  device='',  restore=True)  Instantiate a new variable.", 
            "title": "variable"
        }, 
        {
            "location": "/variables/#get_all_variables", 
            "text": "tflearn.variables.get_all_variables   ()  Get all Graph variables.", 
            "title": "get_all_variables"
        }, 
        {
            "location": "/variables/#get_all_variables_1", 
            "text": "tflearn.variables.get_all_trainable_variable   ()  Get all Graph trainable variables.", 
            "title": "get_all_variables"
        }, 
        {
            "location": "/variables/#get_layer_variables_by_name", 
            "text": "tflearn.variables.get_layer_variables_by_name   (name)  Retrieve a layer's variables, given its name.", 
            "title": "get_layer_variables_by_name"
        }, 
        {
            "location": "/variables/#get_value", 
            "text": "tflearn.variables.get_value   (var,  session=None)  Get a variable's value. If no session provided, use default one.", 
            "title": "get_value"
        }, 
        {
            "location": "/variables/#get_value_1", 
            "text": "tflearn.variables.set_value   (var,  value,  session=None)  Set a variable's value. If no session provided, use default one.", 
            "title": "get_value"
        }, 
        {
            "location": "/helpers/trainer/", 
            "text": "Trainer\n\n\ntflearn.helpers.trainer.Trainer\n  (train_ops,  graph=None,  clip_gradients=5.0,  tensorboard_dir='/tmp/tflearn_logs/',  tensorboard_verbose=0,  checkpoint_path=None,  max_checkpoints=None,  keep_checkpoint_every_n_hours=10000.0,  random_seed=None)\n\n\nGeneric class to handle any TensorFlow graph training. It requires\nthe use of \nTrainOp\n to specify all optimization parameters.\n\n\nArguments\n\n\n\n\n\ntrain_ops\n: list of \nTrainOp\n. A list of a network training\noperations for performing optimizations.\n\n\ngraph\n: \ntf.Graph\n. The TensorFlow graph to use. Default: default tf\ngraph.\n\n\nclip_gradients\n: \nfloat\n. Clip gradient. Default: 5.0.\n\n\ntensorboard_dir\n: \nstr\n. Tensorboard log directory.\nDefault: \"/tmp/tflearn_logs/\".\n\n\ntensorboard_verbose\n: \nint\n. Verbose level. It supports:\n\n\n\n\n0 - Loss, Accuracy. (Best Speed)\n1 - Loss, Accuracy, Gradients.\n2 - Loss, Accuracy, Gradients, Weights.\n3 - Loss, Accuracy, Gradients, Weights, Activations, Sparsity.(Best Visualization)\n\n\n\n\n\n\ncheckpoint_path\n: \nstr\n. Path to store model checkpoints. If None,\nno model checkpoint will be saved. Default: None.\n\n\nmax_checkpoints\n: \nint\n or None. Maximum amount of checkpoints. If\nNone, no limit. Default: None.\n\n\nkeep_checkpoint_every_n_hours\n: \nfloat\n. Number of hours between each\nmodel checkpoints.\n\n\nrandom_seed\n: \nint\n. Random seed, for test reproductivity.\nDefault: None.\n\n\n\n\nMethods\n\n\n\n \n\n\nfit\n  (feed_dicts,  n_epoch=10,  val_feed_dicts=None,  show_metric=False,  snapshot_step=None,  snapshot_epoch=True,  shuffle_all=None,  run_id=None)\n\n\nTrain network with feeded data dicts.\n\n\nExamples\n\n\n\n# 1 Optimizer\ntrainer.fit(feed_dicts={input1: X, output1: Y},val_feed_dicts={input1: X, output1: Y})\ntrainer.fit(feed_dicts={input1: X1, input2: X2, output1: Y},val_feed_dicts=0.1) # 10% of data used for validation\n\n# 2 Optimizers\ntrainer.fit(feed_dicts=[{in1: X1, out1:Y}, {in2: X2, out2:Y2}],val_feed_dicts=[{in1: X1, out1:Y}, {in2: X2, out2:Y2}])\n\n\n\n\nArguments\n\n\n\n\n\nfeed_dicts\n: \ndict\n or list of \ndict\n. The dictionary to feed\ndata to the network. It follows Tensorflow feed dict\nspecifications: '{placeholder: data}'. In case of multiple\noptimizers, a list of dict is expected, that will\nrespectively feed optimizers.\n\n\nn_epoch\n: \nint\n. Number of epoch to runs.\n\n\nval_feed_dicts\n: \ndict\n, list of \ndict\n, \nfloat\n or list of\n\nfloat\n. The data used for validation. Feed dict are\nfollowing the same specification as \nfeed_dicts\n above. It\nis also possible to provide a \nfloat\n for splitting training\ndata for validation.\n\n\nshow_metric\n: \nbool\n. If True, accuracy will be calculated and\ndisplayed at every step. Might give slower training.\n\n\nsnapshot_step\n: \nint\n. If not None, the network will be snapshot\nevery provided step (calculate validation loss/accuracy and\nsave model, if a \ncheckpoint_path\n is specified in \nTrainer\n).\n\n\nsnapshot_epoch\n: \nbool\n. If True, snapshot the network at the end\nof every epoch.\n\n\nshuffle_all\n: \nbool\n. If True, shuffle all data batches (overrides\n\nTrainOp\n shuffle parameter behavior).\n\n\nrun_id\n: \nstr\n. A name for the current run. Used for Tensorboard\ndisplay. If no name provided, a random one will be generated.\n\n\n\n\n \n\n\nrestore\n  (model_file)\n\n\nRestore a Tensorflow model\n\n\nArguments\n\n\n\n\n\nmodel_file\n: path of tensorflow model to restore\n\n\n\n\n \n\n\nsave\n  (model_file,  global_step=None)\n\n\nSave a Tensorflow model\n\n\nArguments\n\n\n\n\n\nmodel_file\n: \nstr\n. Saving path of tensorflow model\n\n\nglobal_step\n: \nfloat\n. The training step to append to the\nmodel file name (optional).\n\n\n\n\n\n\nTrainOp\n\n\ntflearn.helpers.trainer.TrainOp\n  (loss,  optimizer,  metric=None,  batch_size=64,  ema=0.0,  trainable_vars=None,  shuffle=True,  step_tensor=None,  name=None,  graph=None)\n\n\nTrainOp represents a set of operation used for optimizing a network.\n\n\nA TrainOp is meant to hold all training parameters of an optimizer.\n\nTrainer\n class will then instantiate them all specifically considering all\noptimizers of the network (set names, scopes... set optimization ops...).\n\n\nArguments\n\n\n\n\n\nloss\n: \nTensor\n. Loss operation to evaluate network cost.\nOptimizer will use this cost function to train network.\n\n\noptimizer\n: \nOptimizer\n. Tensorflow Optimizer. The optimizer to\nuse to train network.\n\n\nmetric\n:  \nTensor\n. The metric tensor to be used for evaluation.\n\n\nbatch_size\n: \nint\n. Batch size for data feeded to this optimizer.\nDefault: 64.\n\n\nema\n: \nfloat\n. Exponential moving averages.\n\n\ntrainable_vars\n: list of \ntf.Variable\n. List of trainable variables to\nuse for training. Default: all trainable variables.\n\n\nshuffle\n: \nbool\n. Shuffle data.\n\n\nstep_tensor\n: \ntf.Tensor\n. A variable holding training step. If not\nprovided, it will be created. Early defining the step tensor\nmight be useful for network creation, such as for learning rate\ndecay.\n\n\nname\n: \nstr\n. A name for this class (optional).\n\n\ngraph\n: \ntf.Graph\n. Tensorflow Graph to use for training. Default:\ndefault tf graph.\n\n\n\n\nMethods\n\n\n\n \n\n\ninitialize_fit\n  (feed_dict,  val_feed_dict,  show_metric,  summ_writer)\n\n\nInitialize data for feeding the training process. It is meant to\nbe used by \nTrainer\n before starting to fit data.\n\n\nArguments\n\n\n\n\n\nfeed_dict\n: \ndict\n. The data dictionary to feed.\n\n\nval_feed_dict\n: \ndict\n. The validation data dictionary to feed.\n\n\nshow_metric\n: \nbool\n. If True, display accuracy at every step.\n\n\nsumm_writer\n: \nSummaryWriter\n. The summary writer to use for\nTensorboard logging.\n\n\n\n\n \n\n\ninitialize_training_ops\n  (i,  session,  tensorboard_verbose,  clip_gradients)\n\n\nInitialize all ops used for training. Because a network can have\nmultiple optimizers, an id 'i' is allocated to differentiate them.\nThis is meant to be used by \nTrainer\n when initializing all train ops.\n\n\nArguments\n\n\n\n\n\ni\n: \nint\n. This optimizer training process ID.\n\n\nsession\n: \ntf.Session\n. The session used to train the network.\n\n\ntensorboard_verbose\n: \nint\n. Logs verbose. Supports:\n\n\n\n\n0 - Loss, Accuracy.\n1 - Loss, Accuracy, Gradients.\n2 - Loss, Accuracy, Gradients, Weights.\n3 - Loss, Accuracy, Gradients, Weights, Activations, Sparsity..\n\n\n\n\n\n\nclip_gradients\n: \nfloat\n. Option for clipping gradients.", 
            "title": "Trainer"
        }, 
        {
            "location": "/helpers/trainer/#trainer", 
            "text": "tflearn.helpers.trainer.Trainer   (train_ops,  graph=None,  clip_gradients=5.0,  tensorboard_dir='/tmp/tflearn_logs/',  tensorboard_verbose=0,  checkpoint_path=None,  max_checkpoints=None,  keep_checkpoint_every_n_hours=10000.0,  random_seed=None)  Generic class to handle any TensorFlow graph training. It requires\nthe use of  TrainOp  to specify all optimization parameters.", 
            "title": "Trainer"
        }, 
        {
            "location": "/helpers/trainer/#trainop", 
            "text": "tflearn.helpers.trainer.TrainOp   (loss,  optimizer,  metric=None,  batch_size=64,  ema=0.0,  trainable_vars=None,  shuffle=True,  step_tensor=None,  name=None,  graph=None)  TrainOp represents a set of operation used for optimizing a network.  A TrainOp is meant to hold all training parameters of an optimizer. Trainer  class will then instantiate them all specifically considering all\noptimizers of the network (set names, scopes... set optimization ops...).", 
            "title": "TrainOp"
        }, 
        {
            "location": "/helpers/evaluator/", 
            "text": "Evaluator\n\n\ntflearn.helpers.evaluator.Evaluator\n  (tensors,  model=None,  session=None)\n\n\nA class used for performing predictions or evaluate a model performances.\n\n\nArguments\n\n\n\n\n\ntensors\n: list of \nTensor\n. A list of tensors to perform predictions.\n\n\nmodel\n: \nstr\n. The model weights path (Optional).\n\n\nsession\n: \nSession\n. The session to run the prediction (Optional).\n\n\n\n\nMethods\n\n\n\n \n\n\npredict\n  (feed_dict)\n\n\nRun data through each tensor's network, and return prediction value.\n\n\nArguments\n\n\n\n\n\nfeed_dict\n: \ndict\n. Feed data dictionary, with placeholders as\nkeys, and data as values.\n\n\n\n\nReturns\n\n\n\nAn \narray\n. In case of multiple tensors to predict, array is a\nconcatanation of each tensor prediction result.", 
            "title": "Evaluator"
        }, 
        {
            "location": "/helpers/evaluator/#evaluator", 
            "text": "tflearn.helpers.evaluator.Evaluator   (tensors,  model=None,  session=None)  A class used for performing predictions or evaluate a model performances.", 
            "title": "Evaluator"
        }, 
        {
            "location": "/helpers/summarizer/", 
            "text": "summarize_variables\n\n\ntflearn.helpers.summarizer.summarize_variables\n  (train_vars=None,  summary_collection='tflearn_summ')\n\n\nArguemnts:\ntrain_vars: list of \nVariable\n. The variable weights to monitor.\nsummary_collection: A collection to add this summary to and\nalso used for returning a merged summary over all its elements.\nDefault: 'tflearn_summ'.\n\n\nReturns\n\n\n\nTensor\n. Merge of all summary in 'summary_collection'\n\n\n\n\nsummarize_activations\n\n\ntflearn.helpers.summarizer.summarize_activations\n  (activations,  summary_collection='tflearn_summ')\n\n\nArguemnts:\nactivations: list of \nTensor\n. The activations to monitor.\nsummary_collection: A collection to add this summary to and\nalso used for returning a merged summary over all its elements.\nDefault: 'tflearn_summ'.\n\n\nReturns\n\n\n\nTensor\n. Merge of all summary in 'summary_collection'\n\n\n\n\nsummarize_activations\n\n\ntflearn.helpers.summarizer.summarize_gradients\n  (grads,  summary_collection='tflearn_summ')\n\n\nArguemnts:\ngrads: list of \nTensor\n. The gradients to monitor.\nsummary_collection: A collection to add this summary to and\nalso used for returning a merged summary over all its elements.\nDefault: 'tflearn_summ'.\n\n\nReturns\n\n\n\nTensor\n. Merge of all summary in 'summary_collection'\n\n\n\n\nsummarize\n\n\ntflearn.helpers.summarizer.summarize\n  (value,  type,  name,  summary_collection='tflearn_summ')\n\n\nA custom summarization op.\n\n\nArguemnts:\nvalue: \nTensor\n. The tensor value to monitor.\ntype: \nstr\n among 'histogram', 'scalar'. The data monitoring type.\nname: \nstr\n. A name for this summary.\nsummary_collection: A collection to add this summary to and\nalso used for returning a merged summary over all its elements.\nDefault: 'tflearn_summ'.\n\n\nReturns\n\n\n\nTensor\n. Merge of all summary in 'summary_collection'.", 
            "title": "Summarizer"
        }, 
        {
            "location": "/helpers/summarizer/#summarize_variables", 
            "text": "tflearn.helpers.summarizer.summarize_variables   (train_vars=None,  summary_collection='tflearn_summ')  Arguemnts:\ntrain_vars: list of  Variable . The variable weights to monitor.\nsummary_collection: A collection to add this summary to and\nalso used for returning a merged summary over all its elements.\nDefault: 'tflearn_summ'.", 
            "title": "summarize_variables"
        }, 
        {
            "location": "/helpers/summarizer/#summarize_activations", 
            "text": "tflearn.helpers.summarizer.summarize_activations   (activations,  summary_collection='tflearn_summ')  Arguemnts:\nactivations: list of  Tensor . The activations to monitor.\nsummary_collection: A collection to add this summary to and\nalso used for returning a merged summary over all its elements.\nDefault: 'tflearn_summ'.", 
            "title": "summarize_activations"
        }, 
        {
            "location": "/helpers/summarizer/#summarize_activations_1", 
            "text": "tflearn.helpers.summarizer.summarize_gradients   (grads,  summary_collection='tflearn_summ')  Arguemnts:\ngrads: list of  Tensor . The gradients to monitor.\nsummary_collection: A collection to add this summary to and\nalso used for returning a merged summary over all its elements.\nDefault: 'tflearn_summ'.", 
            "title": "summarize_activations"
        }, 
        {
            "location": "/helpers/summarizer/#summarize", 
            "text": "tflearn.helpers.summarizer.summarize   (value,  type,  name,  summary_collection='tflearn_summ')  A custom summarization op.  Arguemnts:\nvalue:  Tensor . The tensor value to monitor.\ntype:  str  among 'histogram', 'scalar'. The data monitoring type.\nname:  str . A name for this summary.\nsummary_collection: A collection to add this summary to and\nalso used for returning a merged summary over all its elements.\nDefault: 'tflearn_summ'.", 
            "title": "summarize"
        }, 
        {
            "location": "/helpers/regularizer/", 
            "text": "add_weights_regularizer\n\n\ntflearn.helpers.regularizer.add_weights_regularizer\n  (variable,  loss='L2',  weight_decay=0.001,  add_to_collection=None)\n\n\nAdd a weights regularizer to the provided Tensor\n\n\nArguments\n\n\n\n\n\nvariable\n: \nVariable\n. Tensor to add regularization.\n\n\nloss\n: \nstr\n. Regularization mode.\n\n\nweight_decay\n: \nfloat\n. Decay to use for regularization.\n\n\nadd_to_collection\n: \nstr\n. Add the regularization loss to the\nspecified collection. Default: tf.GraphKeys.REGULARIZATION_LOSSES.\n\n\n\n\nReturns\n\n\n\ntf.Tensor\n. The weight regularizer.", 
            "title": "Regularizer"
        }, 
        {
            "location": "/helpers/regularizer/#add_weights_regularizer", 
            "text": "tflearn.helpers.regularizer.add_weights_regularizer   (variable,  loss='L2',  weight_decay=0.001,  add_to_collection=None)  Add a weights regularizer to the provided Tensor", 
            "title": "add_weights_regularizer"
        }, 
        {
            "location": "/data_utils/", 
            "text": "to_categorical\n\n\ntflearn.data_utils.to_categorical\n  (y,  nb_classes)\n\n\nConvert class vector (integers from 0 to nb_classes)\nto binary class matrix, for use with categorical_crossentropy.\n\n\nArguments\n\n\n\n\n\ny\n: \narray\n. Class vector to convert.\n\n\nnb_classes\n: \nint\n. Total number of classes.\n\n\n\n\n\n\npad_sequences\n\n\ntflearn.data_utils.pad_sequences\n  (sequences,  maxlen=None,  dtype='int32',  padding='pre',  truncating='pre',  value=0.0)\n\n\nPad each sequence to the same length: the length of the longest sequence.\nIf maxlen is provided, any sequence longer than maxlen is truncated to\nmaxlen. Truncation happens off either the beginning (default) or the\nend of the sequence. Supports post-padding and pre-padding (default).\n\n\nArguments\n\n\n\n\n\nsequences\n: list of lists where each element is a sequence.\n\n\nmaxlen\n: int, maximum length.\n\n\ndtype\n: type to cast the resulting sequence.\n\n\npadding\n: 'pre' or 'post', pad either before or after each sequence.\n\n\ntruncating\n: 'pre' or 'post', remove values from sequences larger than\nmaxlen either in the beginning or in the end of the sequence\n\n\nvalue\n: float, value to pad the sequences to the desired value.\n\n\n\n\nReturns\n\n\n\nx: \nnumpy array\n with dimensions (number_of_sequences, maxlen)\n\n\n\n\nstring_to_semi_redundant_sequences\n\n\ntflearn.data_utils.string_to_semi_redundant_sequences\n  (string,  seq_maxlen=25,  redun_step=3)\n\n\nVectorize a string and returns parsed sequences and targets, along with\nthe associated dictionary.\n\n\nArguments\n\n\n\n\n\npath\n: \nfloat\n. Path of text file.\n\n\nseq_maxlen\n: \nint\n. Maximum length of a sequence. Default: 25.\n\n\nredun_step\n: \nint\n. Redundancy step. Default: 3.\n\n\n\n\nReturns\n\n\n\ntuple\n: (inputs, targets, dictionary)\n\n\n\n\nshuffle\n\n\ntflearn.data_utils.shuffle\n  (*arrs)\n\n\nShuffle given arrays at unison, along first axis.\n\n\nArguments\n\n\n\n\n\n*arrs: Each array to shuffle at unison as a parameter.\n\n\n\n\nReturns\n\n\n\nTuple of shuffled arrays.\n\n\n\n\nsamplewise_zero_center\n\n\ntflearn.data_utils.samplewise_zero_center\n  (X)\n\n\nZero center each sample by subtracting it by its mean.\n\n\nArguments\n\n\n\n\n\nX\n: \narray\n. The batch of samples to center.\n\n\n\n\nReturns\n\n\n\nA numpy array with same shape as input.\n\n\n\n\nsamplewise_std_normalization\n\n\ntflearn.data_utils.samplewise_std_normalization\n  (X)\n\n\nScale each sample with its standard deviation.\n\n\nArguments\n\n\n\n\n\nX\n: \narray\n. The batch of samples to scale.\n\n\n\n\nReturns\n\n\n\nA numpy array with same shape as input.\n\n\n\n\nfeaturewise_zero_center\n\n\ntflearn.data_utils.featurewise_zero_center\n  (X,  mean=None)\n\n\nZero center every sample with specified mean. If not specified, the mean\nis evaluated over all samples.\n\n\nArguments\n\n\n\n\n\nX\n: \narray\n. The batch of samples to center.\n\n\nmean\n: \nfloat\n. The mean to use for zero centering. If not specified, it\nwill be evaluated on provided data.\n\n\n\n\nReturns\n\n\n\nA numpy array with same shape as input. Or a tuple (array, mean) if no\nmean value was specified.\n\n\n\n\nfeaturewise_std_normalization\n\n\ntflearn.data_utils.featurewise_std_normalization\n  (X,  std=None)\n\n\nScale each sample by the specified standard deviation. If no std\nspecified, std is evaluated over all samples data.\n\n\nArguments\n\n\n\n\n\nX\n: \narray\n. The batch of samples to scale.\n\n\nstd\n: \nfloat\n. The std to use for scaling data. If not specified, it\nwill be evaluated over the provided data.\n\n\n\n\nReturns\n\n\n\nA numpy array with same shape as input. Or a tuple (array, std) if no\nstd value was specified.", 
            "title": "Data Utils"
        }, 
        {
            "location": "/data_utils/#to_categorical", 
            "text": "tflearn.data_utils.to_categorical   (y,  nb_classes)  Convert class vector (integers from 0 to nb_classes)\nto binary class matrix, for use with categorical_crossentropy.", 
            "title": "to_categorical"
        }, 
        {
            "location": "/data_utils/#pad_sequences", 
            "text": "tflearn.data_utils.pad_sequences   (sequences,  maxlen=None,  dtype='int32',  padding='pre',  truncating='pre',  value=0.0)  Pad each sequence to the same length: the length of the longest sequence.\nIf maxlen is provided, any sequence longer than maxlen is truncated to\nmaxlen. Truncation happens off either the beginning (default) or the\nend of the sequence. Supports post-padding and pre-padding (default).", 
            "title": "pad_sequences"
        }, 
        {
            "location": "/data_utils/#string_to_semi_redundant_sequences", 
            "text": "tflearn.data_utils.string_to_semi_redundant_sequences   (string,  seq_maxlen=25,  redun_step=3)  Vectorize a string and returns parsed sequences and targets, along with\nthe associated dictionary.", 
            "title": "string_to_semi_redundant_sequences"
        }, 
        {
            "location": "/data_utils/#shuffle", 
            "text": "tflearn.data_utils.shuffle   (*arrs)  Shuffle given arrays at unison, along first axis.", 
            "title": "shuffle"
        }, 
        {
            "location": "/data_utils/#samplewise_zero_center", 
            "text": "tflearn.data_utils.samplewise_zero_center   (X)  Zero center each sample by subtracting it by its mean.", 
            "title": "samplewise_zero_center"
        }, 
        {
            "location": "/data_utils/#samplewise_std_normalization", 
            "text": "tflearn.data_utils.samplewise_std_normalization   (X)  Scale each sample with its standard deviation.", 
            "title": "samplewise_std_normalization"
        }, 
        {
            "location": "/data_utils/#featurewise_zero_center", 
            "text": "tflearn.data_utils.featurewise_zero_center   (X,  mean=None)  Zero center every sample with specified mean. If not specified, the mean\nis evaluated over all samples.", 
            "title": "featurewise_zero_center"
        }, 
        {
            "location": "/data_utils/#featurewise_std_normalization", 
            "text": "tflearn.data_utils.featurewise_std_normalization   (X,  std=None)  Scale each sample by the specified standard deviation. If no std\nspecified, std is evaluated over all samples data.", 
            "title": "featurewise_std_normalization"
        }, 
        {
            "location": "/config/", 
            "text": "init_graph\n\n\ntflearn.config.init_graph\n  (seed=None,  log_device=False,  num_cores=0,  gpu_memory_fraction=0,  soft_placement=True)\n\n\nInitialize a graph with specific parameters.\n\n\nArguments\n\n\n\n\n\nseed\n: \nint\n. Set the graph random seed.\n\n\nlog_device\n: \nbool\n. Log device placement or not.\n\n\nnum_cores\n: Number of CPU cores to be used. Default: All.\n\n\ngpu_memory_fraction\n: A value between 0 and 1 that indicates what\nfraction of the available GPU memory to pre-allocate for each\nprocess. 1 means to pre-allocate all of the GPU memory,\n0.5 means the process allocates ~50% of the available GPU\nmemory. Default: Use all GPU's available memory.\n\n\nsoft_placement\n: \nbool\n. Whether soft placement is allowed. If true,\nan op will be placed on CPU if:1. there's no GPU implementation for the OP - or2. no GPU devices are known or registered - or3. need to co-locate with reftype input(s) which are from CPU.\n\n\n\n\n\n\nis_training\n\n\ntflearn.config.is_training\n  (is_training=False,  session=None)\n\n\nCheck whether the model is currently in training mode or not.\n\n\nReturns\n\n\n\nA \nbool\n, True if training, False else.\n\n\n\n\nget_training_mode\n\n\ntflearn.config.get_training_mode\n  ()\n\n\nReturns variable in-use to set training mode.\n\n\nReturns\n\n\n\nA \nVariable\n, the training mode holder.", 
            "title": "Graph Config"
        }, 
        {
            "location": "/config/#init_graph", 
            "text": "tflearn.config.init_graph   (seed=None,  log_device=False,  num_cores=0,  gpu_memory_fraction=0,  soft_placement=True)  Initialize a graph with specific parameters.", 
            "title": "init_graph"
        }, 
        {
            "location": "/config/#is_training", 
            "text": "tflearn.config.is_training   (is_training=False,  session=None)  Check whether the model is currently in training mode or not.", 
            "title": "is_training"
        }, 
        {
            "location": "/config/#get_training_mode", 
            "text": "tflearn.config.get_training_mode   ()  Returns variable in-use to set training mode.", 
            "title": "get_training_mode"
        }, 
        {
            "location": "/contributions/", 
            "text": "Contributions\n\n\nReport a bug\n\n\nTFLearn is actually at its early stage, there are probably many bugs around... We will be grateful if you would help us to find them. To report a bug, simply open an issue in the GitHub 'issues' section.\n\n\nPull request\n\n\nIf you made improvements to TFLearn or fixed a bug, feel free to send us a pull-request. Please give a brief introduction of the new feature or the bug. When adding new class or functions, make sure that you are following TFLearn docstring syntax.\n\n\nRequest a new feature\n\n\nIf you think about a new feature to improve TFLearn, let us know by opening an issue in GitHub.\n\n\nQuestions\n\n\nTo get help on how to use TFLearn or its functionalities, you can as well open an issue in GitHub.", 
            "title": "Contributions"
        }, 
        {
            "location": "/contributions/#contributions", 
            "text": "", 
            "title": "Contributions"
        }, 
        {
            "location": "/contributions/#report-a-bug", 
            "text": "TFLearn is actually at its early stage, there are probably many bugs around... We will be grateful if you would help us to find them. To report a bug, simply open an issue in the GitHub 'issues' section.", 
            "title": "Report a bug"
        }, 
        {
            "location": "/contributions/#pull-request", 
            "text": "If you made improvements to TFLearn or fixed a bug, feel free to send us a pull-request. Please give a brief introduction of the new feature or the bug. When adding new class or functions, make sure that you are following TFLearn docstring syntax.", 
            "title": "Pull request"
        }, 
        {
            "location": "/contributions/#request-a-new-feature", 
            "text": "If you think about a new feature to improve TFLearn, let us know by opening an issue in GitHub.", 
            "title": "Request a new feature"
        }, 
        {
            "location": "/contributions/#questions", 
            "text": "To get help on how to use TFLearn or its functionalities, you can as well open an issue in GitHub.", 
            "title": "Questions"
        }, 
        {
            "location": "/license/", 
            "text": "MIT License\n\n\nCopyright (c) 2016 TFLearn Contributors.\nEach contributor holds copyright over his own contributions. The project\nversioning keep tracks of such information.\n\n\nBy contributing to the TFLearn repository, the contributor releases their\ncontent to the license and copyright terms herein.\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.", 
            "title": "License"
        }
    ]
}