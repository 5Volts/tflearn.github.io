<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Aymeric Damien">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Core Layers - TFLearn</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Core Layers";
    var mkdocs_page_input_path = "layers/core.md";
    var mkdocs_page_url = "/layers/core/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-81255389-1', 'tflearn.org');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> TFLearn</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../doc_index/">Index</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../installation/">Installation</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../getting_started/">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../tutorials/">Tutorials</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../examples/">Examples</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Models</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/dnn/">Deep Neural Network</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/generator/">Generative Neural Network</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Layers</span></li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">Core Layers</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#input-data">Input Data</a></li>
                
            
                <li class="toctree-l3"><a href="#fully-connected">Fully Connected</a></li>
                
            
                <li class="toctree-l3"><a href="#dropout">Dropout</a></li>
                
            
                <li class="toctree-l3"><a href="#custom-layer">Custom Layer</a></li>
                
            
                <li class="toctree-l3"><a href="#reshape">Reshape</a></li>
                
            
                <li class="toctree-l3"><a href="#flatten">Flatten</a></li>
                
            
                <li class="toctree-l3"><a href="#activation">Activation</a></li>
                
            
                <li class="toctree-l3"><a href="#single-unit">Single Unit</a></li>
                
            
                <li class="toctree-l3"><a href="#fully-connected-highway">Fully Connected Highway</a></li>
                
            
                <li class="toctree-l3"><a href="#one-hot-encoding">One Hot Encoding</a></li>
                
            
                <li class="toctree-l3"><a href="#time-distributed">Time Distributed</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../conv/">Convolutional Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../recurrent/">Recurrent Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../normalization/">Normalization Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../embedding_ops/">Embedding Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../merge_ops/">Merge Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../estimator/">Estimator Layers</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Built-in Ops</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../activations/">Activations</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../objectives/">Objectives</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../optimizers/">Optimizers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../metrics/">Metrics</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../initializations/">Initializations</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../losses/">Losses</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../summaries/">Summaries</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../variables/">Variables</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Data Management</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_utils/">Data Utils</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_preprocessing/">Data Preprocessing</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_augmentation/">Data Augmentation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_flow/">Data Flow</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Others</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../config/">Graph Config</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Helpers for Extending Tensorflow</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/trainer/">Trainer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/evaluator/">Evaluator</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/summarizer/">Summarizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/regularizer/">Regularizer</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contributions/">Contributions</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../license/">License</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">TFLearn</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Core Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/tflearn/tflearn/edit/master/docs/layers/core.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="input-data">Input Data</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.input_data</b></span>  (shape=None,  placeholder=None,  dtype=tf.float32,  data_preprocessing=None,  data_augmentation=None,  name='InputData')</span></p>
<p>This layer is used for inputting (aka. feeding) data to a network.
A TensorFlow placeholder will be used if it is supplied,
otherwise a new placeholder will be created with the given shape.</p>
<p>Either a shape or placeholder must be provided, otherwise an
exception will be raised.</p>
<p>Furthermore, the placeholder is added to TensorFlow collections
so it can be retrieved using tf.get_collection(tf.GraphKeys.INPUTS)
as well as tf.GraphKeys.LAYER_TENSOR + '/' + name. Similarly for
the data preprocessing and augmentation objects which are stored in
the collections with tf.GraphKeys.DATA_PREP and tf.GraphKeys.DATA_AUG.
This allows other parts of TFLearn to easily retrieve and use these
objects by referencing these graph-keys.</p>
<h3>Input</h3>

<p>List of <code>int</code> (Shape), to create a new placeholder.
Or
<code>Tensor</code> (Placeholder), to use an existing placeholder.</p>
<h3>Output</h3>

<p>Placeholder Tensor with given shape.</p>
<h3>Arguments</h3>

<ul>
<li><strong>shape</strong>: list of <code>int</code>. An array or tuple representing input data shape.
It is required if no placeholder is provided. First element should
be 'None' (representing batch size), if not provided, it will be
added automatically.</li>
<li><strong>placeholder</strong>: A Placeholder to use for feeding this layer (optional).
If not specified, a placeholder will be automatically created.
You can retrieve that placeholder through graph key: 'INPUTS',
or the 'placeholder' attribute of this function's returned tensor.</li>
<li><strong>dtype</strong>: <code>tf.type</code>, Placeholder data type (optional). Default: float32.</li>
<li><strong>data_preprocessing</strong>: A <code>DataPreprocessing</code> subclass object to manage
real-time data pre-processing when training and predicting (such
as zero center data, std normalization...).</li>
<li><strong>data_augmentation</strong>: <code>DataAugmentation</code>. A <code>DataAugmentation</code> subclass
object to manage real-time data augmentation while training (
such as random image crop, random image flip, random sequence
reverse...).</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<hr />
<h1 id="fully-connected">Fully Connected</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.fully_connected</b></span>  (incoming,  n_units,  activation='linear',  bias=True,  weights_init='truncated_normal',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  reuse=False,  scope=None,  name='FullyConnected')</span></p>
<p>A fully connected layer.</p>
<h3>Input</h3>

<p>(2+)-D Tensor [samples, input dim]. If not 2D, input will be flatten.</p>
<h3>Output</h3>

<p>2D Tensor [samples, n_units].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming (2+)D Tensor.</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>function</code> (returning a <code>Tensor</code>).
Activation applied to this layer (see tflearn.activations).
Default: 'linear'.</li>
<li><strong>bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>weights_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Weights initialization.
(see tflearn.initializations) Default: 'truncated_normal'.</li>
<li><strong>bias_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Bias initialization.
(see tflearn.initializations) Default: 'zeros'.</li>
<li><strong>regularizer</strong>: <code>str</code> (name) or <code>Tensor</code>. Add a regularizer to this
layer weights (see tflearn.regularizers). Default: None.</li>
<li><strong>weight_decay</strong>: <code>float</code>. Regularizer decay parameter. Default: 0.001.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>restore</strong>: <code>bool</code>. If True, this layer weights will be restored when
loading a model.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
<li><strong>name</strong>: A name for this layer (optional). Default: 'FullyConnected'.</li>
</ul>
<h3>Attributes</h3>

<ul>
<li><strong>scope</strong>: <code>Scope</code>. This layer scope.</li>
<li><strong>W</strong>: <code>Tensor</code>. Variable representing units weights.</li>
<li><strong>b</strong>: <code>Tensor</code>. Variable representing biases.</li>
</ul>
<hr />
<h1 id="dropout">Dropout</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.dropout</b></span>  (incoming,  keep_prob,  noise_shape=None,  name='Dropout')</span></p>
<p>Outputs the input element scaled up by <code>1 / keep_prob</code>. The scaling is so
that the expected sum is unchanged.</p>
<p>By default, each element is kept or dropped independently. If noise_shape
is specified, it must be broadcastable to the shape of x, and only dimensions
with noise_shape[i] == shape(x)[i] will make independent decisions. For
example, if shape(x) = [k, l, m, n] and noise_shape = [k, 1, 1, n], each
batch and channel component will be kept independently and each row and column
will be kept or not kept together.</p>
<h3>Arguments</h3>

<ul>
<li>incoming : A <code>Tensor</code>. The incoming tensor.</li>
<li>keep_prob : A float representing the probability that each element
is kept.</li>
<li>noise_shape : A 1-D Tensor of type int32, representing the shape for
randomly generated keep/drop flags.</li>
<li>name : A name for this layer (optional).</li>
</ul>
<h3>References</h3>

<p>Dropout: A Simple Way to Prevent Neural Networks from Overfitting.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever &amp; R. Salakhutdinov,
(2014), Journal of Machine Learning Research, 5(Jun)(2), 1929-1958.</p>
<h3>Links</h3>

<p><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></p>
<hr />
<h1 id="custom-layer">Custom Layer</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.custom_layer</b></span>  (incoming,  custom_fn,  **kwargs)</span></p>
<p>A custom layer that can apply any operations to the incoming Tensor or
list of <code>Tensor</code>. The custom function can be pass as a parameter along
with its parameters.</p>
<h3>Arguments</h3>

<ul>
<li>incoming : A <code>Tensor</code> or list of <code>Tensor</code>. Incoming tensor.</li>
<li>custom_fn : A custom <code>function</code>, to apply some ops on incoming tensor.</li>
<li>**kwargs: Some custom parameters that custom function might need.</li>
</ul>
<hr />
<h1 id="reshape">Reshape</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.reshape</b></span>  (incoming,  new_shape,  name='Reshape')</span></p>
<p>A layer that reshape the incoming layer tensor output to the desired shape.</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: A <code>Tensor</code>. The incoming tensor.</li>
<li><strong>new_shape</strong>: A list of <code>int</code>. The desired shape.</li>
<li><strong>name</strong>: A name for this layer (optional).</li>
</ul>
<hr />
<h1 id="flatten">Flatten</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.flatten</b></span>  (incoming,  name='Flatten')</span></p>
<p>Flatten the incoming Tensor.</p>
<h3>Input</h3>

<p>(2+)-D <code>Tensor</code>.</p>
<h3>Output</h3>

<p>2-D <code>Tensor</code> [batch, flatten_dims].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. The incoming tensor.</li>
</ul>
<hr />
<h1 id="activation">Activation</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.activation</b></span>  (incoming,  activation='linear',  name='activation')</span></p>
<p>Apply given activation to incoming tensor.</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: A <code>Tensor</code>. The incoming tensor.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>function</code> (returning a <code>Tensor</code>).
Activation applied to this layer (see tflearn.activations).
Default: 'linear'.</li>
</ul>
<hr />
<h1 id="single-unit">Single Unit</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.single_unit</b></span>  (incoming,  activation='linear',  bias=True,  trainable=True,  restore=True,  reuse=False,  scope=None,  name='Linear')</span></p>
<p>A single unit (Linear) Layer.</p>
<h3>Input</h3>

<p>1-D Tensor [samples]. If not 2D, input will be flatten.</p>
<h3>Output</h3>

<p>1-D Tensor [samples].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming Tensor.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>function</code>. Activation applied to this
layer (see tflearn.activations). Default: 'linear'.</li>
<li><strong>bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>restore</strong>: <code>bool</code>. If True, this layer weights will be restored when
loading a model.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
<li><strong>name</strong>: A name for this layer (optional). Default: 'Linear'.</li>
</ul>
<h3>Attributes</h3>

<ul>
<li><strong>W</strong>: <code>Tensor</code>. Variable representing weight.</li>
<li><strong>b</strong>: <code>Tensor</code>. Variable representing bias.</li>
</ul>
<hr />
<h1 id="fully-connected-highway">Fully Connected Highway</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.highway</b></span>  (incoming,  n_units,  activation='linear',  transform_dropout=None,  weights_init='truncated_normal',  bias_init='zeros',  regularizer=None,  weight_decay=0.001,  trainable=True,  restore=True,  reuse=False,  scope=None,  name='FullyConnectedHighway')</span></p>
<p>A fully connected highway network layer, with some inspiration from
<a href="https://github.com/fomorians/highway-fcn">https://github.com/fomorians/highway-fcn</a>.</p>
<h3>Input</h3>

<p>(2+)-D Tensor [samples, input dim]. If not 2D, input will be flatten.</p>
<h3>Output</h3>

<p>2D Tensor [samples, n_units].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming (2+)D Tensor.</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>function</code> (returning a <code>Tensor</code>).
Activation applied to this layer (see tflearn.activations).
Default: 'linear'.</li>
<li><strong>transform_dropout</strong>: <code>float</code>: Keep probability on the highway transform gate.</li>
<li><strong>weights_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Weights initialization.
(see tflearn.initializations) Default: 'truncated_normal'.</li>
<li><strong>bias_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Bias initialization.
(see tflearn.initializations) Default: 'zeros'.</li>
<li><strong>regularizer</strong>: <code>str</code> (name) or <code>Tensor</code>. Add a regularizer to this
layer weights (see tflearn.regularizers). Default: None.</li>
<li><strong>weight_decay</strong>: <code>float</code>. Regularizer decay parameter. Default: 0.001.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>restore</strong>: <code>bool</code>. If True, this layer weights will be restored when
loading a model</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
<li><strong>name</strong>: A name for this layer (optional). Default: 'FullyConnectedHighway'.</li>
</ul>
<h3>Attributes</h3>

<ul>
<li><strong>scope</strong>: <code>Scope</code>. This layer scope.</li>
<li><strong>W</strong>: <code>Tensor</code>. Variable representing units weights.</li>
<li><strong>W_t</strong>: <code>Tensor</code>. Variable representing units weights for transform gate.</li>
<li><strong>b</strong>: <code>Tensor</code>. Variable representing biases.</li>
<li><strong>b_t</strong>: <code>Tensor</code>. Variable representing biases for transform gate.</li>
</ul>
<h3>Links</h3>

<p><a href="https://arxiv.org/abs/1505.00387">https://arxiv.org/abs/1505.00387</a></p>
<hr />
<h1 id="one-hot-encoding">One Hot Encoding</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.one_hot_encoding</b></span>  (target,  n_classes,  on_value=1.0,  off_value=0.0,  name='OneHotEncoding')</span></p>
<p>Transform numeric labels into a binary vector.</p>
<h3>Input</h3>

<p>The Labels Placeholder.</p>
<h3>Output</h3>

<p>2-D Tensor, The encoded labels.</p>
<h3>Arguments</h3>

<ul>
<li><strong>target</strong>: <code>Placeholder</code>. The labels placeholder.</li>
<li><strong>n_classes</strong>: <code>int</code>. Total number of classes.</li>
<li><strong>on_value</strong>: <code>scalar</code>. A scalar defining the on-value.</li>
<li><strong>off_value</strong>: <code>scalar</code>. A scalar defining the off-value.</li>
<li><strong>name</strong>: A name for this layer (optional). Default: 'OneHotEncoding'.</li>
</ul>
<hr />
<h1 id="time-distributed">Time Distributed</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.core.time_distributed</b></span>  (incoming,  fn,  args=None,  scope=None)</span></p>
<p>This layer applies a function to every timestep of the input tensor. The
custom function first argument must be the input tensor at every timestep.
Additional parameters for the custom function may be specified in 'args'
argument (as a list).</p>
<h3>Examples</h3>

<pre><code class="python"># Applying a fully_connected layer at every timestep
x = time_distributed(input_tensor, fully_connected, [64])

# Using a conv layer at every timestep with a scope
x = time_distributed(input_tensor, conv_2d, [64, 3], scope='tconv')
</code></pre>

<h3>Input</h3>

<p>(3+)-D Tensor [samples, timestep, input_dim].</p>
<h3>Output</h3>

<p>(3+)-D Tensor [samples, timestep, output_dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. The incoming tensor.</li>
<li><strong>fn</strong>: <code>function</code>. A function to apply at every timestep. This function
first parameter must be the input tensor per timestep. Additional
parameters may be specified in 'args' argument.</li>
<li><strong>args</strong>: <code>list</code>. A list of parameters to use with the provided function.</li>
<li><strong>scope</strong>: <code>str</code>. A scope to give to each timestep tensor. Useful when
sharing weights. Each timestep tensor scope will be generated
as 'scope'-'i' where i represents the timestep id. Note that your
custom function will be required to have a 'scope' parameter.</li>
</ul>
<h3>Returns</h3>

<p>A Tensor.</p>
<hr />
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../conv/" class="btn btn-neutral float-right" title="Convolutional Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../models/generator/" class="btn btn-neutral" title="Generative Neural Network"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/tflearn/tflearn" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../../models/generator/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../conv/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>

</body>
</html>
